{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取谣言原文和非谣言原文和谣言评论和非谣言评论文件的全部路径\n",
    "def get_data_dir(target_path):\n",
    "    rumor_class_dirs = os.listdir(target_path)\n",
    "\n",
    "    rumor_file_dir_list = [] #谣言数据根目录\n",
    "    non_rumor_file_dir_list = [] #非谣言数据根目录\n",
    "\n",
    "    #解析谣言和非谣言的数据目录\n",
    "    for filename in rumor_class_dirs:\n",
    "        rumor_file_dir_list.append(os.path.join(target_path, filename, \"rumours\"))\n",
    "        non_rumor_file_dir_list.append(os.path.join(target_path, filename, \"non-rumours\"))\n",
    "\n",
    "    all_non_rumor_content_file_dir_list = [] #所有谣言评论数据目录（根的下下一级目录）\n",
    "    all_rumor_content_file_dir_list = []\n",
    "    all_rumor_srctweet_file_dir_list = [] #所有谣言原文数据目录（根的下下一级目录）\n",
    "    all_non_rumor_srctweet_file_dir_list = []\n",
    "    a_rumor_content_dir = []\n",
    "    a_non_rumor_content_dir = []\n",
    "    \n",
    "    for files in rumor_file_dir_list: \n",
    "        files_dir = os.listdir(files)\n",
    "        for file_dir_files in files_dir:\n",
    "            all_rumor_content_file_dir_list.append(os.path.join(files, file_dir_files, \"reactions\"))\n",
    "            all_rumor_srctweet_file_dir_list.append(os.path.join(files, file_dir_files, \"source-tweet\"))\n",
    "\n",
    "    for files in non_rumor_file_dir_list:\n",
    "        file_dir = os.listdir(files)\n",
    "        for file_dir_files in file_dir:\n",
    "            all_non_rumor_content_file_dir_list.append(os.path.join(files, file_dir_files, \"reactions\"))\n",
    "            all_non_rumor_srctweet_file_dir_list.append(os.path.join(files, file_dir_files, \"source-tweet\"))\n",
    "            \n",
    "    return all_rumor_srctweet_file_dir_list, all_rumor_content_file_dir_list, all_non_rumor_srctweet_file_dir_list, all_non_rumor_content_file_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评论列表，形式为[[content1, content2...], [content1, content2...],]\n",
    "def read_content_data(data_path):\n",
    "    a_twt_content = [] # 每一条原贴的评论列表\n",
    "    # data_src = []\n",
    "    for data_path in data_path:\n",
    "        data = []\n",
    "        for filename in os.listdir(data_path):\n",
    "            with open(os.path.join(data_path, filename), 'r', encoding='utf8') as f:\n",
    "                content = f.read()\n",
    "                data.append(content)\n",
    "        a_twt_content.append(data)\n",
    "        # data_dict = json.loads(content)\n",
    "        # data.append([data_dict[\"text\"]])\n",
    "    return a_twt_content  # data[[content, label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据，通过文件的路径读取，label为谣言和非谣言的标签\n",
    "def read_data(data_dir_list, label):\n",
    "    data = []\n",
    "    # data_src = []\n",
    "    for dir in data_dir_list:\n",
    "        for filename in os.listdir(dir):\n",
    "            with open(os.path.join(dir, filename), 'r', encoding='utf8') as f:\n",
    "                content = f.read()\n",
    "                data.append([content, label])\n",
    "            # data_dict = json.loads(content)\n",
    "            # data.append([data_dict[\"text\"]])\n",
    "    return data  # data[[content, label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将谣言数据整理成[['src_twt': 原贴, ‘label’标签, 'content'评论]],再存入json\n",
    "path = r'D:\\NoteBook\\RumerDetection\\data\\pheme-rnr-dataset'\n",
    "all_rumor_srctwt_list, all_rumor_content_list, all_non_rumor_srctwt_file_dir_list, all_non_rumor_content_file_dir_list = get_data_dir(\n",
    "            path)\n",
    "rumor_srctwt_data = read_data(all_rumor_srctwt_list, 1)  # 读取谣言原贴，原贴上传数据库\n",
    "rumor_content_data = read_content_data(all_rumor_content_list)\n",
    "non_rumor_srctwt_data = read_data(all_non_rumor_srctwt_file_dir_list, 0)\n",
    "non_rumor_content_data = read_content_data(all_non_rumor_content_file_dir_list)\n",
    "all_data = rumor_srctwt_data + non_rumor_srctwt_data\n",
    "all_content = rumor_content_data + non_rumor_content_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(rumor_srctwt_data, rumor_content_data, filename):\n",
    "    twt_con = [] #原题和评论一起打包\n",
    "    for src_twt, contents in zip(rumor_srctwt_data, rumor_content_data):\n",
    "        a_twt_con = []\n",
    "        a_twt_con.append(src_twt[0])\n",
    "        a_twt_con.append(str(src_twt[1]))\n",
    "        for item in contents:\n",
    "            a_twt_con.append(item)\n",
    "        twt_con.append(a_twt_con)\n",
    "    with codecs.open(filename, \"w\", \"utf-8\") as f:\n",
    "        j = json.dumps(twt_con)\n",
    "        f.write(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写入的路径\n",
    "filename = r'D:\\NoteBook\\RumerDetection\\data\\charliehebdo20.json'\n",
    "write_json(all_data[:20], all_content[:20], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(filename, 'r', encoding='utf8') as f:\n",
    "    data = f.read()\n",
    "    data = json.loads(data)\n",
    "    print(data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552789647966109696\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(data[0][3])['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
