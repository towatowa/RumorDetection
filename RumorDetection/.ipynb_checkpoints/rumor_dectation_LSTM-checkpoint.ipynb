{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¯»å–æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import sys\n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "def loadData(filename):\n",
    "    data = []\n",
    "    fr = open(filename, 'r', encoding='utf8')\n",
    "    for line in fr.readlines():                 #é€è¡Œè¯»å–\n",
    "        lineArr = line.strip().split('\\t')      #æ»¤é™¤è¡Œé¦–è¡Œå°¾ç©ºæ ¼ï¼Œä»¥\\tä½œä¸ºåˆ†éš”ç¬¦ï¼Œå¯¹è¿™è¡Œè¿›è¡Œåˆ†è§£\n",
    "        num = np.shape(lineArr)[0]     \n",
    "        data.append([\"\".join(lineArr[0:num-1]), int(lineArr[num-1])])#è¿™ä¸€è¡Œçš„é™¤æœ€åä¸€ä¸ªè¢«æ·»åŠ ä¸ºæ•°æ®\n",
    "        #labelMat.append(int(lineArr[num-1]))#è¿™ä¸€è¡Œçš„æœ€åä¸€ä¸ªæ•°æ®è¢«æ·»åŠ ä¸ºæ ‡ç­¾\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charlie Hebdos Last Tweet Spoofed ISIS Leader AlBaghdadi  URL 0\n",
      "Trocadero square in Paris evacuated Policemen pointing their gun URL 1\n",
      "held by gunman at kosher supermarket in Paris as nd hostagetaking underway AP Gunman linked to Thursdays killing of policewoman 0\n"
     ]
    }
   ],
   "source": [
    "train_data = loadData(\"./data/train_data.txt\")\n",
    "test_data = loadData(\"./data/test_data.txt\")\n",
    "\n",
    "\"test txt len: \", len(test_data), \"train txt len: \", len(train_data)\n",
    "for i in train_data[0:3]:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å»åœç”¨è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ†è¯å’Œå»åœç”¨è¯\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from string import punctuation as enpunctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-3553520d760c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mzhonPunctuation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mu'''ï¼‚ ï¼ƒ ï¼„ ï¼… ï¼† ï¼‡ ï¼ˆ ï¼‰ ï¼Š ï¼‹ ï¼Œ ï¼ ï¼ ï¼š ï¼› ï¼œ ï¼ ï¼ ï¼  ï¼» ï¼¼ ï¼½ ï¼¾ ï¼¿ ï½€ ï½› ï½œ ï½ ï½ ï½Ÿ ï½  ï½¢ ï½£ ï½¤  ã€ƒ ã€ˆ ã€‰ ã€Š ã€‹ ã€Œ ã€ ã€ ã€ ã€ ã€‘ ã€” ã€• ã€– ã€— ã€˜ ã€™ ã€š ã€› ã€œ ã€ ã€ ã€Ÿ  ã€¾ ã€¿ â€“ â€” â€˜ â€™ â€› â€œ â€ â€ â€Ÿ â€¦ â€§ ï¹ ï¹‘ ï¹” Â· ï¼ ï¼Ÿ ï½¡ â†’ ã€ ã€‚'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpunctuations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menpunctuation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzhonPunctuation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#å»æ ‡ç‚¹ç¬¦å·\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msrctweet1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mst\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msrctweet2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mst\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-3553520d760c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mzhonPunctuation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mu'''ï¼‚ ï¼ƒ ï¼„ ï¼… ï¼† ï¼‡ ï¼ˆ ï¼‰ ï¼Š ï¼‹ ï¼Œ ï¼ ï¼ ï¼š ï¼› ï¼œ ï¼ ï¼ ï¼  ï¼» ï¼¼ ï¼½ ï¼¾ ï¼¿ ï½€ ï½› ï½œ ï½ ï½ ï½Ÿ ï½  ï½¢ ï½£ ï½¤  ã€ƒ ã€ˆ ã€‰ ã€Š ã€‹ ã€Œ ã€ ã€ ã€ ã€ ã€‘ ã€” ã€• ã€– ã€— ã€˜ ã€™ ã€š ã€› ã€œ ã€ ã€ ã€Ÿ  ã€¾ ã€¿ â€“ â€” â€˜ â€™ â€› â€œ â€ â€ â€Ÿ â€¦ â€§ ï¹ ï¹‘ ï¹” Â· ï¼ ï¼Ÿ ï½¡ â†’ ã€ ã€‚'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpunctuations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menpunctuation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzhonPunctuation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#å»æ ‡ç‚¹ç¬¦å·\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msrctweet1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mst\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msrctweet2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mst\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "zhonPunctuation = u'''ï¼‚ ï¼ƒ ï¼„ ï¼… ï¼† ï¼‡ ï¼ˆ ï¼‰ ï¼Š ï¼‹ ï¼Œ ï¼ ï¼ ï¼š ï¼› ï¼œ ï¼ ï¼ ï¼  ï¼» ï¼¼ ï¼½ ï¼¾ ï¼¿ ï½€ ï½› ï½œ ï½ ï½ ï½Ÿ ï½  ï½¢ ï½£ ï½¤  ã€ƒ ã€ˆ ã€‰ ã€Š ã€‹ ã€Œ ã€ ã€ ã€ ã€ ã€‘ ã€” ã€• ã€– ã€— ã€˜ ã€™ ã€š ã€› ã€œ ã€ ã€ ã€Ÿ  ã€¾ ã€¿ â€“ â€” â€˜ â€™ â€› â€œ â€ â€ â€Ÿ â€¦ â€§ ï¹ ï¹‘ ï¹” Â· ï¼ ï¼Ÿ ï½¡ â†’ ã€ ã€‚'''\n",
    "punctuations = set([str(i) for i in enpunctuation]) | set([str(i) for i in zhonPunctuation]) #å»æ ‡ç‚¹ç¬¦å·\n",
    "srctweet1 = [nltk.word_tokenize(st[0]) for st in train_data if st[0] != '' and st[0] not in punctuations]\n",
    "srctweet2 = [nltk.word_tokenize(st[0]) for st in test_data if st[0] != '' and st[0] not in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ˜³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ğŸ˜’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ğŸ™ˆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ğŸ˜³ğŸ™ˆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ğŸ˜¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stopword\n",
       "0        ğŸ˜³\n",
       "1        ğŸ˜’\n",
       "2        ğŸ™ˆ\n",
       "3       ğŸ˜³ğŸ™ˆ\n",
       "4        ğŸ˜¬"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=pd.read_csv(\"./data/stop_words-master/english.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ¸…é™¤æ ‡ç‚¹ç¬¦å·\n",
    "def del_mark(word):\n",
    "    punc = '~`!#$%^&*()_+-=|\\';\":/.,?><~Â·ï¼@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰â€”â€”+-=â€œï¼šâ€™ï¼›ã€ã€‚ï¼Œï¼Ÿã€‹ã€Š{\\]}\\[âœ'\n",
    "    return re.sub(r\"[%s]+\" %punc, \"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [['T', ' ', ' ', ' ', 'P', ' ', ' ', 'P', ' ', ' ', ' ', ' ', 'U', 'R', 'L'],\n",
       "  1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_stopwords(srctweet,stopwords):\n",
    "    srctweet_clean = []\n",
    "    all_words = []\n",
    "    for lines in srctweet:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            word = del_mark(word)\n",
    "            line_clean.append(word)\n",
    "            all_words.append(word)##è®°å½•æ‰€æœ‰line_cleanä¸­çš„è¯\n",
    "        srctweet_clean.append(line_clean)\n",
    "    return srctweet_clean,all_words\n",
    "    #print (contents_clean)\n",
    "        \n",
    "\n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "srctweet1,all_words = drop_stopwords(train_data,stopwords)\n",
    "srctweet2,_  = drop_stopwords(test_data,stopwords)\n",
    "i=0\n",
    "for line in srctweet1:\n",
    "    trian_data[i][0] = \" \".join(line)\n",
    "    i=0\n",
    "for line in srctweet2:\n",
    "    test_data[i][0] = \" \".join(line)\n",
    "type(train_data[0]), train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_imdb(data):\n",
    "    \"\"\"\n",
    "    data: list of [string, label]\n",
    "    \"\"\"\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    return [tokenizer(review) for review, _ in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®åˆ†å¥½è¯çš„è®­ç»ƒæ•°æ®é›†æ¥åˆ›å»ºè¯å…¸äº†ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œè¿‡æ»¤æ‰äº†å‡ºç°æ¬¡æ•°å°‘äº5çš„è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-3da9fdeb103c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_vocab_imdb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;34m'# words in vocab:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-3da9fdeb103c>\u001b[0m in \u001b[0;36mget_vocab_imdb\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_vocab_imdb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtokenized_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokenized_imdb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mst\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_data\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-c03f1789ca69>\u001b[0m in \u001b[0;36mget_tokenized_imdb\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-58-c03f1789ca69>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-58-c03f1789ca69>\u001b[0m in \u001b[0;36mtokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "def get_vocab_imdb(data):\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "'# words in vocab:', len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ä¸ºæ¯æ¡è¯„è®ºé•¿åº¦ä¸ä¸€è‡´æ‰€ä»¥ä¸èƒ½ç›´æ¥ç»„åˆæˆå°æ‰¹é‡ï¼Œæˆ‘ä»¬å®šä¹‰preprocess_imdbå‡½æ•°å¯¹æ¯æ¡è¯„è®ºè¿›è¡Œåˆ†è¯ï¼Œå¹¶é€šè¿‡è¯å…¸è½¬æ¢æˆè¯ç´¢å¼•ï¼Œç„¶åé€šè¿‡æˆªæ–­æˆ–è€…è¡¥0æ¥å°†æ¯æ¡è¯„è®ºé•¿åº¦å›ºå®šæˆ300ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb(data, vocab):\n",
    "    max_l = 300  # å°†æ¯æ¡è¯„è®ºé€šè¿‡æˆªæ–­æˆ–è€…è¡¥0ï¼Œä½¿å¾—é•¿åº¦å˜æˆ300\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆ›å»ºæ•°æ®è¿­ä»£å™¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬åˆ›å»ºæ•°æ®è¿­ä»£å™¨ã€‚æ¯æ¬¡è¿­ä»£å°†è¿”å›ä¸€ä¸ªå°æ‰¹é‡çš„æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-a551ae444d78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpreprocess_imdb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpreprocess_imdb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-354bf57b9cf5>\u001b[0m in \u001b[0;36mpreprocess_imdb\u001b[1;34m(data, vocab)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax_l\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_l\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_l\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtokenized_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokenized_imdb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscore\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-c03f1789ca69>\u001b[0m in \u001b[0;36mget_tokenized_imdb\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-58-c03f1789ca69>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-58-c03f1789ca69>\u001b[0m in \u001b[0;36mtokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°ç¬¬ä¸€ä¸ªå°æ‰¹é‡æ•°æ®çš„å½¢çŠ¶ä»¥åŠè®­ç»ƒé›†ä¸­å°æ‰¹é‡çš„ä¸ªæ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 300]) y torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches:', 30)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œæ¯ä¸ªè¯å…ˆé€šè¿‡åµŒå…¥å±‚å¾—åˆ°ç‰¹å¾å‘é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œå¯¹ç‰¹å¾åºåˆ—è¿›ä¸€æ­¥ç¼–ç å¾—åˆ°åºåˆ—ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ç¼–ç çš„åºåˆ—ä¿¡æ¯é€šè¿‡å…¨è¿æ¥å±‚å˜æ¢ä¸ºè¾“å‡ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å°†åŒå‘é•¿çŸ­æœŸè®°å¿†åœ¨æœ€åˆæ—¶é—´æ­¥å’Œæœ€ç»ˆæ—¶é—´æ­¥çš„éšè—çŠ¶æ€è¿ç»“ï¼Œä½œä¸ºç‰¹å¾åºåˆ—çš„è¡¨å¾ä¼ é€’ç»™è¾“å‡ºå±‚åˆ†ç±»ã€‚åœ¨ä¸‹é¢å®ç°çš„BiRNNç±»ä¸­ï¼ŒEmbeddingå®ä¾‹å³åµŒå…¥å±‚ï¼ŒLSTMå®ä¾‹å³ä¸ºåºåˆ—ç¼–ç çš„éšè—å±‚ï¼ŒLinearå®ä¾‹å³ç”Ÿæˆåˆ†ç±»ç»“æœçš„è¾“å‡ºå±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # bidirectionalè®¾ä¸ºTrueå³å¾—åˆ°åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, \n",
    "                                hidden_size=num_hiddens, \n",
    "                                num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        # åˆå§‹æ—¶é—´æ­¥å’Œæœ€ç»ˆæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä½œä¸ºå…¨è¿æ¥å±‚è¾“å…¥\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputsçš„å½¢çŠ¶æ˜¯(æ‰¹é‡å¤§å°, è¯æ•°)ï¼Œå› ä¸ºLSTMéœ€è¦å°†åºåˆ—é•¿åº¦(seq_len)ä½œä¸ºç¬¬ä¸€ç»´ï¼Œæ‰€ä»¥å°†è¾“å…¥è½¬ç½®å\n",
    "        # å†æå–è¯ç‰¹å¾ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º(è¯æ•°, æ‰¹é‡å¤§å°, è¯å‘é‡ç»´åº¦)\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        # rnn.LSTMåªä¼ å…¥è¾“å…¥embeddingsï¼Œå› æ­¤åªè¿”å›æœ€åä¸€å±‚çš„éšè—å±‚åœ¨å„æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚\n",
    "        # outputså½¢çŠ¶æ˜¯(è¯æ•°, æ‰¹é‡å¤§å°, 2 * éšè—å•å…ƒä¸ªæ•°)\n",
    "        outputs, _ = self.encoder(embeddings) # output, (h, c)\n",
    "        # è¿ç»“åˆå§‹æ—¶é—´æ­¥å’Œæœ€ç»ˆæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä½œä¸ºå…¨è¿æ¥å±‚è¾“å…¥ã€‚å®ƒçš„å½¢çŠ¶ä¸º\n",
    "        # (æ‰¹é‡å¤§å°, 4 * éšè—å•å…ƒä¸ªæ•°)ã€‚\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ›å»ºä¸€ä¸ªå«ä¸¤ä¸ªéšè—å±‚çš„åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¯»å–è¯åµŒå…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_idx_list = loadData(\"./data/token_to_idx.txt\")\n",
    "token_to_idx = {}\n",
    "for item in token_to_idx_list:\n",
    "    dic = {item[0]: item[1]}\n",
    "    token_to_idx.update(dic)\n",
    "\n",
    "token_to_idx['happening']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net1 = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(token_to_idx), embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(token_to_idx), embedding_dim=embed_size)\n",
    ")\n",
    "net1 = torch.load(\"./data/net.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2327, 100]), 2327)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1[0].weight.data.shape, len(token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 396 oov words.\n"
     ]
    }
   ],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"ä»é¢„è®­ç»ƒå¥½çš„vocabä¸­æå–å‡ºwordså¯¹åº”çš„è¯å‘é‡\"\"\"\n",
    "    W = pretrained_vocab.weight.data\n",
    "    embed = torch.zeros(len(words),  W.shape[1]) # åˆå§‹åŒ–ä¸º0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = token_to_idx[word]\n",
    "            embed[i, :] = W[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "net.embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.itos, net1[0]))\n",
    "net.embedding.weight.requires_grad = False # ç›´æ¥åŠ è½½é¢„è®­ç»ƒå¥½çš„, æ‰€ä»¥ä¸éœ€è¦æ›´æ–°å®ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®­ç»ƒå¹¶è¯„ä»·æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.4548, train acc 0.798, test acc 0.818, time 2.3 sec\n",
      "epoch 2, loss 0.1620, train acc 0.858, test acc 0.842, time 1.7 sec\n",
      "epoch 3, loss 0.0887, train acc 0.895, test acc 0.842, time 1.7 sec\n",
      "epoch 4, loss 0.0558, train acc 0.915, test acc 0.837, time 1.6 sec\n",
      "epoch 5, loss 0.0389, train acc 0.924, test acc 0.856, time 1.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "# è¦è¿‡æ»¤æ‰ä¸è®¡ç®—æ¢¯åº¦çš„embeddingå‚æ•°\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    \"\"\"sentenceæ˜¯è¯è¯­çš„åˆ—è¡¨\"\"\"\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    #return 'positive' if label.item() == 1 else 'negative'\n",
    "    return int(1) if label.item() == 1 else int(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æµ‹è¯•æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, int)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data[0][0]), type(test_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ†è¯å’Œå»åœç”¨è¯\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from string import punctuation as enpunctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhonPunctuation = u'''ï¼‚ ï¼ƒ ï¼„ ï¼… ï¼† ï¼‡ ï¼ˆ ï¼‰ ï¼Š ï¼‹ ï¼Œ ï¼ ï¼ ï¼š ï¼› ï¼œ ï¼ ï¼ ï¼  ï¼» ï¼¼ ï¼½ ï¼¾ ï¼¿ ï½€ ï½› ï½œ ï½ ï½ ï½Ÿ ï½  ï½¢ ï½£ ï½¤  ã€ƒ ã€ˆ ã€‰ ã€Š ã€‹ ã€Œ ã€ ã€ ã€ ã€ ã€‘ ã€” ã€• ã€– ã€— ã€˜ ã€™ ã€š ã€› ã€œ ã€ ã€ ã€Ÿ  ã€¾ ã€¿ â€“ â€” â€˜ â€™ â€› â€œ â€ â€ â€Ÿ â€¦ â€§ ï¹ ï¹‘ ï¹” Â· ï¼ ï¼Ÿ ï½¡ â†’ ã€ ã€‚'''\n",
    "punctuations = set([str(i) for i in enpunctuation]) | set([str(i) for i in zhonPunctuation]) #å»æ ‡ç‚¹ç¬¦å·\n",
    "srctweet = [nltk.word_tokenize(st[0]) for st in test_data if st[0] != '' and st[0] not in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ˜³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ğŸ˜’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ğŸ™ˆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ğŸ˜³ğŸ™ˆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ğŸ˜¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stopword\n",
       "0        ğŸ˜³\n",
       "1        ğŸ˜’\n",
       "2        ğŸ™ˆ\n",
       "3       ğŸ˜³ğŸ™ˆ\n",
       "4        ğŸ˜¬"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=pd.read_csv(\"./data/stop_words-master/english.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ¸…é™¤æ ‡ç‚¹ç¬¦å·\n",
    "def del_mark(word):\n",
    "    punc = '~`!#$%^&*()_+-=|\\';\":/.,?><~Â·ï¼@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰â€”â€”+-=â€œï¼šâ€™ï¼›ã€ã€‚ï¼Œï¼Ÿã€‹ã€Š{\\]}\\[âœ'\n",
    "    return re.sub(r\"[%s]+\" %punc, \"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " ['We',\n",
       "  'condemn',\n",
       "  'horrible',\n",
       "  'shooting',\n",
       "  'CharlieHebdo',\n",
       "  'We',\n",
       "  'stand',\n",
       "  'freedom',\n",
       "  'speech',\n",
       "  'expression'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_stopwords(srctweet,stopwords):\n",
    "    srctweet_clean = []\n",
    "    all_words = []\n",
    "    for line in srctweet:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            word = del_mark(word)\n",
    "            line_clean.append(word)\n",
    "            all_words.append(word)##è®°å½•æ‰€æœ‰line_cleanä¸­çš„è¯\n",
    "        srctweet_clean.append(line_clean)\n",
    "    return srctweet_clean,all_words\n",
    "    #print (contents_clean)\n",
    "        \n",
    "\n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "srctweet_clean,all_words = drop_stopwords(srctweet,stopwords)\n",
    "type(srctweet_clean[0]), srctweet_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = torch.load(r\"D:\\NoteBook\\RumerDetection\\data\\LSTM_net_loss=0.0389_acc=0.856.pt\")\n",
    "net2.embedding.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = []\n",
    "for item in srctweet_clean:\n",
    "    test_result.append([item, predict_sentiment(net2, vocab, item)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_result[0]), type(test_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('æ­£ç¡®æ•°:', 170, 'æ€»æ•°: ', 209, 'æ­£ç¡®ç‡ï¼š', 81.3397129186603)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(0, len(test_result)):\n",
    "    if int(test_result[i][1]) ^ test_data[i][1] == 0:\n",
    "        count += 1\n",
    "'æ­£ç¡®æ•°:', count, 'æ€»æ•°: ', len(test_result), 'æ­£ç¡®ç‡ï¼š', 1.0 * count/len(test_result) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './data/LSTM_net_loss=0.0389_acc=0.856.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data:  ['UPDATE', 'Reports', 'Sydney', 'Opera', 'House', 'evacuated', 'News'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Sydney', 'siege', 'police', 'enter', 'cafe', 'hostages', 'flee', 'scene', 'gunfire', 'heard', 'reports', 'injured', 'person', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Map', 'locating', 'Paris', 'offices', 'satirical', 'magazine', 'Charlie', 'Hebdo', 'AFP', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['At', 'hostages', 'flee', 'lindtcafe', 'Sydney', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Plane', 'crashes', 'southern', 'France', 'board', 'Germanwings', 'budget', 'airline', 'flying', 'Barcelona', 'Dusseldorf', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['COMING', 'UP', 'LIVE', 'Ottawa', 'shooting', 'Stephen', 'Harper', 'nation', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['NHL', 'postpones', 'Wednesdays', 'LeafsSenators', 'tragedy', 'Ottawa', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Police', 'entered', 'Sydney', 'cafe', 'loud', 'bangs', 'flashes', 'stream', 'sydneysiege'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['GermanWings', 'copilot', 'fresh', 'Muslim', 'convert', 'URLConverts', 'Islam', 'disturbed', 'WhiteMuslims'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['sydneysiege', 'misnomer', 'Its', 'hostage', 'situation', 'Inside', 'cafe', 'You', 'siege', 'Gaza'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Recap', 'Parliament', 'Hill', 'lockdown', 'uniformed', 'Canadian', 'soldier', 'shot', 'War', 'Memorial', 'Suspect', 'loose', 'witnesses', 'rifle'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['MORE', 'Ottawa', 'shootings', 'suspect', 'identified', 'Michael', 'ZehafBibeau', 'unclear', 'shooters', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['How', 'Arab', 'media', 'responded', 'Charlie', 'Hebdo', 'attack', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Police', 'confirm', 'deaths', 'Frances', 'celebrated', 'cartoonists', 'Charb', 'Cabu', 'Wolinski', 'Tignous', 'CharlieHebdo', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['One', 'shooting', 'victim', 'succumbed', 'injuries', 'He', 'Canadian', 'Forces', 'Our', 'prayers', 'loved'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['CharlieHebdo', 'cartoonists', 'killed', 'Charb', 'Cabu', 'Tignous', 'Georges', 'WolinskiJeSuisCharlie', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'A', 'spokesperson', 'The', 'Ottawa', 'Hospital', 'confirms', 'patients', 'stable'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Alert', 'press', 'gallery', 'email', 'RCMP', 'advises', 'downtown', 'Ottawa', 'windows', 'roofs', 'ongoing', 'police', 'incident'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'We', 'reports', 'crash', 'Airbus', 'A', 'Germanwings', 'Barcelona', 'Dusseldorf'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Incidents', 'occurred', 'National', 'War', 'Memorial', 'Rideau', 'Centre', 'Parliament', 'Hill', 'morning', 'ottnews'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Andreas', 'Lubitz', 'copilot', 'Germanwings', 'flight', 'intentionally', 'commended', 'FAA', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['My', 'hostage', 'situation', 'Sydney', 'hurt'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['unprecedented', 'Paris', 'road', 'Porte', 'Vincennes', 'hostage', 'Armed', 'police', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['UPDATE', 'Reports', 'gunman', 'requesting', 'speak', 'Australian', 'leaders', 'radio', 'specialist', 'officers', 'contact', 'News'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['An', 'baby', 'hostages', 'Kosher', 'supermarket', 'Paris', 'Unbearably', 'sad', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['JUST', 'IN', 'women', 'Sydney', 'cafe', 'ongoing', 'hostage', 'situation', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Breaking', 'Shots', 'fired', 'northeast', 'Paris', 'believed', 'Charlie', 'Hebdo', 'suspects', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['RT', 'This', 'media', 'These', 'cleaning', 'riot', 'Ferguson', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Ferguson', 'cops', 'beat', 'charged', 'wproperty', 'damage', 'bleeding', 'uniforms', 'Then', 'lied', 'court', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Police', 'confirm', 'shootings', 'Ottawa', 'PM', 'safe', 'downtown', 'buildings', 'lockdown', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['The', 'correct', 'descriptor', 'shooter', 'radical', 'muslim', 'terrorist', 'ottnews', 'ottawashooting', 'cdnpoli'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['If', 'selfies', 'MartinPlace', 'sydneysiege'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'Germanwings', 'CEO', 'plane', 'minute', 'descent', 'crashing'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Boom', 'RT', 'Why', 'Ferguson', 'PD', 'suspect', 'hours', 'Michael', 'Brown', 'killed', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['NSW', 'Police', 'Police', 'urge', 'media', 'responsible', 'reporting', 'Speculation', 'unnecessary', 'alarm', 'SydneySiege'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Updated', 'An', 'armed', 'person', 'hostages', 'Sydney', 'cafe', 'displayed', 'black', 'flag', 'Arabic', 'script', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['RT', 'If', 'support', 'freedom', 'fighter', 'StopIslamists', 'FreedomOfSpeech', 'CharlieHebdo', 'RedNationRising', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'French', 'media', 'reporting', 'suspects', 'CharlieHebdo', 'attack', 'killed', 'More', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Military', 'helicopters', 'dropping', 'forces', 'fields', 'DammartinenGoele', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['This', 'photo', 'reported', 'suspected', 'Ottawa', 'shooter', 'Michael', 'Zehar', 'Bibeau', 'suspended', 'ISIS', 'Twitter', 'account', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['People', 'gather', 'silence', 'remember', 'Paris', 'shooting', 'victims', 'JeSuisCharlie', 'Photos', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Hostagetaker', 'kosher', 'market', 'Paris', 'killed', 'Mayor', 'confirms', 'brothers', 'killed', 'separate', 'hostagetaking', 'CharlieHebdo'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Banksy', 'reacts', 'CharlieHebdo', 'attack', 'poignant', 'drawing', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['MORE', 'Operation', 'detain', 'massacre', 'suspects', 'unfolding', 'DammartinenGoele', 'miles', 'northeast', 'Paris', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['DEVELOPING', 'Pilot', 'locked', 'cockpit', 'Alps', 'crash', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Canada', 'journalist', 'captures', 'outbreak', 'smallarms', 'Parliament', 'building', 'Ottawa', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Family', 'Michael', 'Brown', 'slams', 'Ferguson', 'police', 'chiefs', 'devious', 'blame', 'victim', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Five', 'escaped', 'Lindt', 'cafe', 'Two', 'female', 'hostages', 'moments', 'sydneysiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['President', 'Obamas', 'statement', 'CharlieHebdo', 'shooting', 'mention', 'freedom', 'speech', 'press'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['REMINDER', 'MikeBrown', 'kid', 'walk', 'street', 'And', 'HE', 'IS', 'DEAD', 'Those', 'Ferguson'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['French', 'official', 'attack', 'Paris', 'weekly', 'CharlieHebdo', 'Kalachnikov', 'automatic', 'rifles', 'rocket', 'launcher', 'Unprecedented', 'AFP'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['JUST', 'IN', 'Gunman', 'Sydney', 'cafe', 'siege', 'devices', 'city', 'Report', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['feels', 'weird', 'hearing', 'media', 'fountain', 'European', 'freedom', 'speech', 'CharlieHebdo', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['SYDNEY', 'Reports', 'hands', 'window', 'black', 'flag', 'Arabic', 'writing', 'MORE', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['PHOTOS', 'New', 'Yorkers', 'write', 'peaceful', 'protest', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Chairs', 'piled', 'door', 'This', 'surreal', 'RTgrahamctv', 'Shot', 'caucus', 'shooting', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'Gunman', 'SydneySiege', 'police', 'devices', 'city', 'demands', 'speak', 'Prime', 'Minister', 'acc', 'reports'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['You', 'offence', 'disapprove', 'silence', 'kill', 'We', 'intimidated', 'JeSuisCharlie', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['RIGHTNOW', 'bridges', 'Ottawa', 'Gatineau', 'NOW', 'CLOSED', 'Active', 'search', 'underway', 'est', 'ParliamentHill', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['agree', 'Ill', 'defend', 'death', 'Voltaire', 'CharlieHebdo', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Police', 'pursuit', 'multiple', 'shooters', 'Ottawa', 'PM', 'safe', 'downtown', 'buildings', 'lockdown', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['FRANCE', 'PHOTO', 'Ahmed', 'Merabet', 'French', 'Muslim', 'Cop', 'victim', 'CharlieHebdo', 'attack', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Can', 'terrorist', 'cunts', 'Paris', 'fucking', 'bullshit'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['The', 'person', 'killed', 'Charlie', 'Hebdo', 'attacks', 'Muslim', 'police', 'officer', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['At', 'gunman', 'SydneySiege', 'image', 'suspect', 'verified', 'No', 'injuries', 'police', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['support', 'cartoons', 'language', 'publishers', 'BUT', 'CharlieHebdo', 'attack', 'sickening', 'dead', 'terrorattack'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Police', 'thrown', 'crowd', 'Not', 'Protester', 'bullets', 'Ferguson'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Update', 'Telegraph', 'reporter', 'Henry', 'Samuels', 'tells', 'Sky', 'News', 'killed', 'shooting', 'Charlie', 'Hebdo', 'headquarters', 'Paris'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Killed', 'Charlie', 'Hebdo', 'suspects', 'firing', 'security', 'forces'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Explosions', 'gunfire', 'heard', 'scene', 'hostagetaking', 'kosher', 'supermarket', 'eastern', 'Paris', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Hobby', 'Lobby', 'Ferguson', 'PD', 'clarification', 'We', 'chopping', 'someones', 'hand', 'maximum', 'penalty', 'shoplifting'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['It', 'St', 'Louis', 'County', 'police', 'officer', 'shot', 'PIO', 'Brian', 'Schellman', 'ferguson'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Canada', 'Soldier', 'shot', 'MT', 'On', 'Sunday', 'handsome', 'guard', 'picture', 'friend', 'RIP', 'Nathan', 'Cirillo', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Australian', 'police', 'cordon', 'Sydney', 'street', 'reports', 'hostages', 'cafe', 'Flag', 'window', 'ISIS', 'militants'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['RT', 'The', 'STL', 'SWAT', 'team', 'weapons', 'drawn', 'Ferguson', 'protest', 'broad', 'daylight', 'media', 'attending', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['ctvottawa', 'confirms', 'separate', 'shootings', 'One', 'Parliament', 'Hill', 'National', 'War', 'Memorial', 'Rideau', 'Centre'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['sydneysiege', 'A', 'loud', 'blasts', 'bursts', 'ammunition', 'heard', 'cafe', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['RIPNathanCirilloRT', 'Soldier', 'killed', 'war', 'memorial', 'identified', 'Cpl', 'Nathan', 'Cirillo', 'OttawaShooting', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Ottawa', 'police', 'confirm', 'suspect', 'Parliament', 'Hill', 'shooting', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['killed', 'Paris', 'Alu', 'Akhbar', 'shouting', 'armed', 'attack', 'satirical', 'mag', 'Charlie', 'Hebdo', 'Background', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['One', 'suspect', 'CharlieHebdo', 'shooting', 'handed', 'remain', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Gutwrenching', 'soldier', 'guarding', 'national', 'war', 'memorial', 'died', 'Rest', 'peace', 'Cpl', 'Nathan', 'Cirillo', 'OttawaShooting', 'NeverForget'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Reports', 'SydneySiege', 'gunman', 'speak', 'Tony', 'Abbott', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Gunfire', 'explosions', 'heard', 'armed', 'police', 'stormed', 'Lindt', 'cafe', 'building', 'Sydney', 'sydneysiege'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['US', 'Army', 'increases', 'security', 'Tomb', 'Unknown', 'Soldier', 'Arlington', 'Natl', 'Cemetery', 'precaution', 'Ottawa', 'shootings'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['After', 'minutes', 'Chief', 'Jackson', 'abruptly', 'press', 'questions', 'yelled', 'MikeBrown', 'Ferguson'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['This', 'stand', 'victims', 'condemn', 'terror', 'remind', 'Breivik', 'Mcveigh', 'CharlieHebdo'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Theres', 'anger', 'selfies', 'sydneysiege', 'cafe', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Thoughts', 'prayers', 'hometown', 'Ottawa'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['A', 'siege', 'Sydney', 'trend', 'jihadist', 'violence', 'reached', 'Australia', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Sydney', 'bad', 'hostage', 'situation'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Tonights', 'Maple', 'Leafs', 'Senators', 'postponed', 'shootings', 'Ottawa', 'morning', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Uber', 'Is', 'Allegedly', 'Charging', 'Passengers', 'Minimum', 'During', 'Sydney', 'Siege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Sydneysiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['What', 'Sheikh', 'Haron', 'suspected', 'hostage', 'taker', 'SydneySiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Thoughts', 'prayer', 'involved', 'tragic', 'events', 'morning', 'Ottawa'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Children', 'hostage', 'kosher', 'store', 'reports', 'ParisAttacks', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Thoughts', 'Ferguson', 'We', 'collectively', 'stand', 'senseless', 'violence', 'confronts', 'youth', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['VIDEO', 'Australian', 'police', 'talking', 'gunman', 'holding', 'hostages', 'cafe', 'Sydney', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'A', 'confirms', 'CP', 'deceased', 'soldier', 'Ottawa', 'Cpl', 'Nathan', 'Cirillo', 'Cirillo', 'Hamilton', 'Argylls'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Hostages', 'Sydney', 'Australia', 'cafe', 'Islamic', 'flag', 'displayed', 'window', 'local', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Minutes', 'silence', 'France', 'remember', 'victims', 'Paris', 'shooting', 'JeSuisCharlie', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Let', 'pray', 'hostages', 'MartinPlace', 'Sydney', 'police', 'handling', 'situation', 'peaceful', 'resolution'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['The', 'violence', 'OttawaShooting', 'city', 'edge', 'After', 'chaos', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Sydneysiege', 'Police', 'direct', 'contact', 'gunman', 'precise', 'hostages', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Germanwings', 'confirm', 'distress', 'signal', 'flight', 'U', 'crashed', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['First', 'MikeBrown', 'accused', 'shoplifting', 'Now', 'robbery', 'Its', 'telling', 'Ferguson', 'PD', 'spewing', 'lie', 'lie'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Parisians', 'gather', 'Notre', 'Dame', 'mourn', 'CharlieHebdo', 'killings', 'Jesuischarlie', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Third', 'defending', 'rights', 'Charlie', 'Hebdo', 'defended', 'defending', 'speech', 'attacking', 'Muslims'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Very', 'tense', 'situation', 'Ottawa', 'morning', 'Multiple', 'gun', 'shots', 'fired', 'caucus', 'safe', 'lockdown', 'Unbelievable'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['CBC', 'News', 'Ottawa', 'independently', 'confirmed', 'gunman', 'shot', 'killed', 'Michael', 'ZehafBibeau', 'cbcOTT', 'OTTnews'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Germanwings', 'CEO', 'Germans', 'Spanish', 'citizens', 'Americans', 'plane', 'crashed', 'France', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Hypocrites', 'screen', 'shot', 'CharlieHebdo', 'PK', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['The', 'AFP', 'hostage', 'taker', 'monitoring', 'social', 'media', 'Guardian', 'Australia', 'sources', 'sydneysiege'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Artists', 'fight', 'violence', 'images', 'CharlieHebdo', 'solidarity', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Ottawa', 'police', 'Actively', 'suspects', 'Canadian', 'parliament', 'shooting', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Reports', 'shots', 'fired', 'Parliament', 'Hill'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['believed', 'hostages', 'Paris', 'market', 'tells', 'BFMTV', 'Interior', 'Ministry', 'confirmed', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['RT', 'RCMP', 'advises', 'downtown', 'Ottawa', 'windows', 'roofs', 'ongoing', 'police', 'incident'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Banksy', 'JeSuisCharlie', 'beautiful', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Someone', 'eyes', 'checkthedatestamp', 'photo', 'Isnt', 'JUNE', 'Ferguson', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['controversial', 'covers', 'CharlieHebdo', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'French', 'police', 'convoy', 'helicopters', 'rush', 'scene', 'operation', 'detain', 'CharlieHebdo', 'shooting', 'suspects', 'AP'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Ferguson', 'police', 'chief', 'announced', 'officer', 'Darren', 'Wilson', 'shot', 'unarmed', 'teen', 'Michael', 'Brown'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['This', 'exclusion', 'zone', 'Sydneys', 'CBD', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['French', 'officials', 'alleged', 'gunman', 'ID', 'getaway', 'CharlieHebdo', 'attack', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'confirms', 'PAX', 'Crew', 'board'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Hes', 'blame', 'menHe', 'gun', 'blame', 'gunownersHes', 'Muslim', 'blame', 'Muslims', 'sydneysiege'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'One', 'gunman', 'killed', 'shooting', 'Parliament', 'Hill', 'sources', 'confirm', 'CTV', 'News'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Paramedics', 'carrying', 'hostages', 'MartinPl', 'SydneySiege', 'News'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Police', 'officers', 'hold', 'moment', 'silence', 'remember', 'killed', 'CharlieHebdo', 'attack', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['The', 'Germanwings', 'AirbusA', 'headed', 'Barcelona', 'Spain', 'Dusseldorf', 'Germany', 'French', 'Prime', 'Minister', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Well', 'forget', 'brave', 'women', 'protect', 'capital', 'Thank', 'Ottawa', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['As', 'involved', 'SydneySiege', 'Praying', 'wake', 'â¤'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Reports', 'explosion', 'kebab', 'mosque', 'Lyon', 'No', 'indication', 'link', 'CharlieHebdo', 'attack', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Snipers', 'Ottawas', 'National', 'Art', 'Gallery', 'gunfire', 'parliament', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['WATCH', 'The', 'moment', 'women', 'escaped', 'sydneysiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Sydney', 'Hostage', 'taker', 'demands', 'ISIS', 'flag', 'warns', 'bombs', 'city', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Extended', 'Video', 'Guns', 'raised', 'officers', 'rush', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['No', 'surprise', 'In', 'July', 'windowsmashing', 'rioters', 'stormed', 'Paris', 'chanting', 'gas', 'Jews', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Police', 'confirm', 'shooting', 'Rideau', 'Centre', 'mall', 'downtown', 'Ottawa', 'east', 'Parliament', 'Hill', 'cdnpoli'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Hostage', 'siege', 'selfies', 'RT', 'The', 'punters', 'selfies', 'Lindt', 'Cafe', 'sydneysiege', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['The', 'terrorist', 'attack', 'Paris', 'obscenity', 'It', 'violent', 'assault', 'freedom', 'media', 'freed', 'expression', 'peoples'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['carried', 'CharlieHebdo', 'attack', 'authorities', 'searching', 'French', 'interior', 'minister', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Another', 'hostage', 'lindtcafe', 'sydneysiege', 'demands', 'gunmen', 'blacked', 'media', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Rideau', 'Centre', 'manager', 'shooting', 'mall', 'But', 'leave', 'enter', 'OttawaShooting'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['They', 'bring', 'France', 'knees', 'stand', 'JeSuisCharlie', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'NEWS', 'PHOTO', 'GALLERY', 'Parliament', 'Hill', 'attacked', 'soldier', 'shot', 'National', 'War', 'Memorial', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['At', 'hostages', 'kosher', 'supermarket', 'eastern', 'Paris', 'reports', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Reports', 'Sydney', 'Australia', 'cafe', 'ISIS', 'flag', 'displayed', 'hostages', 'window', 'See', 'tweets'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Hostages', 'Sydney', 'cafe', 'Islamic', 'flag', 'window', 'local', 'TV', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['AP', 'VIDEO', 'Gunman', 'shoots', 'Canadian', 'soldier', 'standing', 'guard', 'National', 'War', 'Memorial', 'Ottawa', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['AirbusCrash', 'According', 'Germanwings', 'plane', 'crashed', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['French', 'media', 'calling', 'CharlieHebdo', 'attack', 'attack', 'France', 'decades', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Cops', 'forcing', 'legally', 'stand', 'sidewalk', 'Ferguson'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Armed', 'French', 'Police', 'At', 'Trocadero', 'In', 'Paris', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['From', 'Martin', 'Place', 'newsroom', 'gunman', 'rotating', 'hostages', 'forcing', 'stand', 'windows', 'hours'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['NBCNews', 'demand', 'retract', 'lie', 'Ferguson', 'shouting', 'Kill', 'police', 'Reporters', 'scene', 'refuted'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Still', 'conflicting', 'reports', 'death', 'witness', 'Police', 'Tempers', 'rising', 'Ferguson', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Smoke', 'building', 'Charlie', 'Hebdo', 'attack', 'suspects', 'located', 'WATCH', 'LIVE', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Frances', 'interior', 'minister', 'operation', 'detain', 'Charlie', 'Hebdo', 'suspects', 'underway', 'northeast', 'Paris', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Both', 'Charlie', 'Hebdo', 'suspects', 'killed', 'police', 'storm', 'building', 'police', 'sources', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['The', 'person', 'killed', 'Charlie', 'Hebdo', 'attacks', 'Muslim', 'police', 'officer', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['journalists', 'massacres', 'Israel', 'Gaza', 'Where', 'freedom', 'speech', 'CharlieHebdo', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'hostages', 'escaping', 'sydneysiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Last', 'Germanwings', 'A', 'distress', 'emergency', 'emergency', 'authorities', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['French', 'Prime', 'Minister', 'confirming', 'plane', 'crash', 'U'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['UPDATE', 'hostages', 'Sydney', 'coffee', 'Sydney', 'Opera', 'House', 'Abbott', 'deliver', 'statement', 'shortly', 'ISISAttacks', 'sydneyseige'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Things', 'ugly', 'A', 'racist', 'bloke', 'yelling', 'rest', 'crowd', 'Muhammed', 'pedophile', 'rapist', 'sydneysiege', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Ferguson', 'cops', 'beat', 'innocent', 'charged', 'bleeding', 'uniforms', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Reports', 'police', 'identified', 'hostage', 'taker', 'SydneySiege', 'Five', 'hostages', 'escaped', 'believed'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['The', 'copilot', 'Germanwings', 'Airbus', 'convert', 'Islam', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['NORAD', 'increases', 'planes', 'alert', 'status', 'ready', 'respond', 'official', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Total', 'hostages', 'Lindt', 'MartinPlace', 'women', 'LIVE', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Sydney', 'Australia', 'Not', 'technically', 'ISIS', 'flag', 'Shahadah', 'flag', 'Islamic', 'creed', 'Commonly', 'militants', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Canada', 'Michael', 'ZehafBibeau', 'suspect', 'Ottawa', 'shooting', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['If', 'Germanwings', 'copilot', 'Lubitz', 'Muslimit', 'wud', 'hv', 'classic', 'terrorism', 'caseBut', 'brownso', 'mental', 'heath'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Paris', 'gunman', 'vows', 'kill', 'hostages', 'France', 'storms', 'printing', 'plant', 'brothers', 'accused', 'attacking', 'newspaper', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['If', 'Sydney', 'Texas', 'customers', 'drawn', 'guns', 'moment', 'ISIS', 'terrorist', 'walked', 'A'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'Soldier', 'shot', 'Canadian', 'parliament', 'authorities', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['If', 'Sydney', 'err', 'safety', 'Also', 'cautious', 'Lindt', 'cafe', 'diversion'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Exactly', 'Why', 'release', 'stopping', 'Mike', 'Brown', 'robbery', 'JESUS', 'CHRIST', 'THIS', 'IS', 'BAD', 'Ferguson'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Apparent', 'hostage', 'situation', 'unfolding', 'Sydney', 'prime', 'minister', 'convenes', 'security', 'briefing', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['babies', 'passengers', 'board', 'crashed', 'Germanwings', 'plane', 'briefing', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Dear', 'confused', 'rightwing', 'friends', 'article', 'Mondays', 'incident', 'Ottawa', 'shooting', 'morning', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['SYDNEYSIEGE', 'OVER', 'police', 'confirm', 'Reports', 'hostagetaker', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Ottawa', 'police', 'confirm', 'male', 'shooting', 'suspect', 'died', 'custody', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Incredible', 'shot', 'A', 'woman', 'mouthtomouth', 'fallen', 'soldier', 'War', 'Memorial', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['FACT', 'We', 'MikeBrown', 'allegedly', 'stole', 'cheap', 'cigars', 'Ferguson', 'police', 'officer', 'shot', 'killed'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['A', 'flight', 'U', 'registration', 'DAIPX', 'lost', 'feet', 'UTC', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['JUST', 'IN', 'Ferguson', 'police', 'identify', 'officer', 'shot', 'Michael', 'Brown', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['A', 'French', 'police', 'officer', 'suspected', 'Charlie', 'Hebdo', 'gunmen', 'martyrs', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['These', 'hostages', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Ferguson', 'PD', 'charged', 'HenryDavis', 'destruction', 'property', 'bleeding', 'uniforms', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['When', 'MikeBrown', 'shot', 'police', 'officer', 'Darren', 'Wilson', 'Ferguson', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Brilliant', 'CharlieHebdo', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['No', 'word', 'gunman', 'Man', 'Monis', 'SydneySiege', 'News', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Ferguson', 'police', 'chief', 'robbery', 'DOES', 'NOT', 'initial', 'contact', 'Darren', 'Wilson', 'Michael', 'Brown'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Top', 'Qatar', 'urges', 'Muslims', 'â€˜', 'apologize', 'CharlieHebdo', 'ParisShooting', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Reports', 'Alleged', 'Sydney', 'hostage', 'taker', 'charged', 'accessory', 'exwifes', 'murder', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Line', 'police', 'cars', 'beams', 'greets', 'enter', 'Ferguson', 'Its', 'shut', 'No', 'media', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['All', 'European', 'flags', 'fly', 'halfmast', 'pay', 'tribute', 'victims', 'terrorist', 'attack', 'Paris', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'Anonymous', 'audio', 'files', 'police', 'dispatch', 'EMS', 'MikeBrown', 'shooting', 'Will', 'release', 'ASAP', 'Ferguson'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'UPDATE', 'hostages', 'leave', 'Sydneysiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Germanwings', 'flight', 'U', 'crashes', 'French', 'Alps', 'plane', 'altitude', 'mayday', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Police', 'confirm', 'sydneysiege', 'Two', 'reportedly', 'injured', 'suffering', 'gunshot', 'wounds', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['The', 'US', 'Consulate', 'Sydney', 'evacuated', 'Statement', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Banksys', 'CharlieHebdo', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Looks', 'Charlie', 'Hebdo', 'cartoonist', 'killed', 'Charb', 'AlQaedas', 'hit', 'list', 'Cabu', 'Wolinski', 'Tignous'] ,  lable:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(test_result)):\n",
    "    if int(test_result[i][1]) != test_data[i][1]:\n",
    "        print('test_data: ', srctweet_clean[i], ', ', 'lable: ', test_result[i][1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
