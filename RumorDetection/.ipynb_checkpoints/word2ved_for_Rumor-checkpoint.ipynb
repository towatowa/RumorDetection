{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–è°£è¨€åŸæ–‡å’Œéè°£è¨€åŸæ–‡å’Œè°£è¨€è¯„è®ºå’Œéè°£è¨€è¯„è®ºæ–‡ä»¶çš„å…¨éƒ¨è·¯å¾„\n",
    "def get_data_dir(target_path):\n",
    "    rumor_class_dirs = os.listdir(target_path)\n",
    "\n",
    "    rumor_file_dir_list = [] #è°£è¨€æ•°æ®æ ¹ç›®å½•\n",
    "    non_rumor_file_dir_list = [] #éè°£è¨€æ•°æ®æ ¹ç›®å½•\n",
    "\n",
    "    #è§£æè°£è¨€å’Œéè°£è¨€çš„æ•°æ®ç›®å½•\n",
    "    for filename in rumor_class_dirs:\n",
    "        rumor_file_dir_list.append(os.path.join(target_path, filename, \"rumours\"))\n",
    "        non_rumor_file_dir_list.append(os.path.join(target_path, filename, \"non-rumours\"))\n",
    "\n",
    "    all_non_rumor_content_file_dir_list = [] #æ‰€æœ‰è°£è¨€è¯„è®ºæ•°æ®ç›®å½•ï¼ˆæ ¹çš„ä¸‹ä¸‹ä¸€çº§ç›®å½•ï¼‰\n",
    "    all_rumor_content_file_dir_list = []\n",
    "    all_rumor_srctweet_file_dir_list = [] #æ‰€æœ‰è°£è¨€åŸæ–‡æ•°æ®ç›®å½•ï¼ˆæ ¹çš„ä¸‹ä¸‹ä¸€çº§ç›®å½•ï¼‰\n",
    "    all_non_rumor_srctweet_file_dir_list = []\n",
    "    \n",
    "    for files in rumor_file_dir_list: \n",
    "        files_dir = os.listdir(files)\n",
    "        for file_dir_files in files_dir:\n",
    "            all_rumor_content_file_dir_list.append(os.path.join(files, file_dir_files, \"reactions\"))\n",
    "            all_rumor_srctweet_file_dir_list.append(os.path.join(files, file_dir_files, \"source-tweet\"))\n",
    "\n",
    "    for files in non_rumor_file_dir_list:\n",
    "        file_dir = os.listdir(files)\n",
    "        for file_dir_files in file_dir:\n",
    "            all_non_rumor_content_file_dir_list.append(os.path.join(files, file_dir_files, \"reactions\"))\n",
    "            all_non_rumor_srctweet_file_dir_list.append(os.path.join(files, file_dir_files, \"source-tweet\"))\n",
    "            \n",
    "    return all_rumor_srctweet_file_dir_list, all_rumor_content_file_dir_list, all_non_rumor_srctweet_file_dir_list, all_non_rumor_content_file_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\NoteBook\\RumerDetection\\data\\pheme-rnr-dataset\\charliehebdo\\rumours\\552783238415265792\\source-tweet D:\\NoteBook\\RumerDetection\\data\\pheme-rnr-dataset\\charliehebdo\\non-rumours\\552784600502915072\\source-tweet\n"
     ]
    }
   ],
   "source": [
    "target_path = r\"D:\\NoteBook\\RumerDetection\\data\\pheme-rnr-dataset\"\n",
    "all_rumor_srctweet_list, _, all_non_rumor_srctweet_list, _ = get_data_dir(target_path)\n",
    "print(all_rumor_srctweet_list[0], all_non_rumor_srctweet_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è¯»å–æ•°æ®ï¼Œé€šè¿‡æ–‡ä»¶çš„è·¯å¾„è¯»å–ï¼Œlabelä¸ºè°£è¨€å’Œéè°£è¨€çš„æ ‡ç­¾\n",
    "def read_data(data_dir_list, label):\n",
    "    data = []\n",
    "    for dir in data_dir_list:\n",
    "        for filename in os.listdir(dir):\n",
    "            with open(os.path.join(dir, filename), 'r', encoding='utf8') as f:\n",
    "                content = f.read()\n",
    "            data_dict = json.loads(content)\n",
    "            data.append([data_dict[\"text\"], label])\n",
    "    return data #data[[content, label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è°£è¨€çš„æ–‡æœ¬æ€»æ•°ï¼š 1972\n",
      "éè°£è¨€çš„æ–‡æœ¬æ€»æ•°: 3830\n"
     ]
    }
   ],
   "source": [
    "#è·å–è°£è¨€æ•°æ®å’Œéè°£è¨€æ•°æ®ï¼Œå¹¶è¾“å‡ºè°£è¨€å’Œéè°£è¨€çš„æ–‡æœ¬æ•°é‡\n",
    "all_rumor_data = read_data(all_rumor_srctweet_list, 1)\n",
    "all_non_rumor_data = read_data(all_non_rumor_srctweet_list, 0)\n",
    "print(\"è°£è¨€çš„æ–‡æœ¬æ€»æ•°ï¼š\",  len(all_rumor_data))\n",
    "print(\"éè°£è¨€çš„æ–‡æœ¬æ€»æ•°:\", len(all_non_rumor_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import emoji\n",
    "import unicodedata\n",
    "\n",
    "#emojiã€boxDrawingã€Faceï¼šhttps://apps.timwhitlock.info/emoji/tables/unicode#block-6c-other-additional-symbols\n",
    "import re\n",
    "# è¿‡æ»¤emojiæ›´å…¨çš„æ–¹æ³•\n",
    "#pip install emoji\n",
    " \n",
    "def filterEmoji(desstr,restr=' '):\n",
    "    # è¿‡æ»¤emoji\n",
    "    try:\n",
    "        co = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "    except re.error:\n",
    "        co = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "    return co.sub(restr, desstr)\n",
    " \n",
    "def filterBoxDrawing(desstr, restr=' '):\n",
    "    # è¿‡æ»¤å½¢å¦‚ï¼šâ• ã€â•¤ç­‰boxdrawingå­—ç¬¦\n",
    "    co = re.compile(u'[\\u2500-\\u257f]')\n",
    "    return co.sub(restr, desstr)\n",
    " \n",
    "def filterFace(desstr, restr= ' '):\n",
    "    # è¿‡æ»¤ï¼šå½¢å¦‚[è¡°]ã€[ç”Ÿæ°”]ã€[å¼€å¿ƒ]ã€[æ‚è„¸]ç­‰è¡¨æƒ…ï¼Œç”¨è¯å…¸æ›´å¥½äº›\n",
    "    p = re.compile('\\[.{1,4}\\]')\n",
    "    t = p.findall(desstr)\n",
    "    for i in t:\n",
    "        desstr = desstr.replace(i, restr)\n",
    "    return desstr\n",
    " \n",
    "def filterSpecialSym(desstr, restr=' '):\n",
    "    #print u'1\\u20e3\\ufe0f' #10ä¸ªç‰¹æ®Šçš„ç±»ä¼¼emoijçš„è¡¨æƒ…\n",
    "    co = re.compile(u'[0-9]?\\u20e3\\ufe0f?')\n",
    "    return co.sub(restr, desstr)\n",
    " \n",
    "def bodyNorm(body):\n",
    "    #body = re.compile(u'''\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n''').sub(' ', body) # å¾—ç”¨16ä¸ªæ–œæ æ‰è¡Œéœ‡æƒŠ\n",
    "    body = re.compile(u'''\\\\\\\\+?n''').sub(' ', body)\n",
    "    body = filterSpecialSym(body)\n",
    "    body = filterEmoji(body)\n",
    "    body = filterBoxDrawing(body)\n",
    "    body = filterFace(body)\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ ‡ç‚¹ç¬¦å·å’Œå»è¡¨æƒ…å®éªŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å‰: dhadhh 8 * 9.dapağŸ˜³ğŸ˜’ğŸ™ˆğŸ˜³ğŸ™ˆ''/..///.1__===1- dadkhh\n",
      "å¤„ç†å: dhadhh 8 * 9.dapa     ''/..///.1__===1- dadkhh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\towa\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.613 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†è¯å: dhadhh 8 9 dapa .. 1 __ 1 dadkhh\n"
     ]
    }
   ],
   "source": [
    "contentbody = \"dhadhh 8 * 9.dapağŸ˜³ğŸ˜’ğŸ™ˆğŸ˜³ğŸ™ˆ''/..///.1__===1- dadkhh\"\n",
    "#ucontentbody = str(contentbody, 'utf-8')\n",
    "print('å¤„ç†å‰:',contentbody)\n",
    "body = bodyNorm(contentbody)\n",
    "print('å¤„ç†å:',body)\n",
    " \n",
    "import jieba\n",
    "from string import punctuation as enpunctuation\n",
    "zhonPunctuation = u'''ï¼‚ ï¼ƒ ï¼„ ï¼… ï¼† ï¼‡ ï¼ˆ ï¼‰ ï¼Š ï¼‹ ï¼Œ ï¼ ï¼ ï¼š ï¼› ï¼œ ï¼ ï¼ ï¼  ï¼» ï¼¼ ï¼½ ï¼¾ ï¼¿ ï½€ ï½› ï½œ ï½ ï½ ï½Ÿ ï½  ï½¢ ï½£ ï½¤  ã€ƒ ã€ˆ ã€‰ ã€Š ã€‹ ã€Œ ã€ ã€ ã€ ã€ ã€‘ ã€” ã€• ã€– ã€— ã€˜ ã€™ ã€š ã€› ã€œ ã€ ã€ ã€Ÿ  ã€¾ ã€¿ â€“ â€” â€˜ â€™ â€› â€œ â€ â€ â€Ÿ â€¦ â€§ ï¹ ï¹‘ ï¹” Â· ï¼ ï¼Ÿ ï½¡ â†’ ã€ ã€‚'''\n",
    "punctuations = set([str(i) for i in enpunctuation]) | set([str(i) for i in zhonPunctuation])\n",
    "words = [seg for seg in jieba.cut(body) if seg!=' ' and seg not in punctuations]\n",
    "print('åˆ†è¯å:',' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å°†æ•°æ®åˆå¹¶åå¹¶æ‰“æ•£\n",
    "all_data = all_rumor_data + all_non_rumor_data\n",
    "random.shuffle(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#å°†æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†,åˆ†ä¸ºå¸¦æ ‡ç­¾å’Œä¸å¸¦æ ‡ç­¾æ€»å…±å››ä¸ªæ–‡ä»¶\n",
    "def all_data_save(train, all_data, islabel, txtname):\n",
    "    # åœ¨ç”Ÿæˆall_data.txtä¹‹å‰ï¼Œé¦–å…ˆå°†å…¶æ¸…ç©º\n",
    "    filename = train + '_' + txtname\n",
    "    data_path = './data/' + filename\n",
    "    with open(data_path, 'w') as f:\n",
    "        f.seek(0)\n",
    "        f.truncate()\n",
    "    count = 0\n",
    "    punc = '\\]~`!#$%^&*()_+-=|\\';\"\":/.,?><~Â·ï¼@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰â€”â€”+-=â€œï¼šâ€™ï¼›ã€ã€‚ï¼Œï¼Ÿã€‹ã€Š{}1234567890â†’âœ”ï¸\\[âœ'\n",
    "    if islabel == 1 and train == 'train':\n",
    "        with open(data_path, 'a',encoding='UTF-8') as f:\n",
    "            for data in  all_data:\n",
    "                if count > int(0.9 * len(all_data)):#è®­ç»ƒé›†å’Œæµ‹è¯•é›†9:1\n",
    "                    break\n",
    "                count += 1\n",
    "                #print(type(data))\n",
    "                data = [str(i) for i in data]\n",
    "                #data[0] = re.sub('@\\w+|http.*\\w+|\\n|[%s]+' %punc,\"\", data[0])\n",
    "                data[0] = re.sub('http.*',\" URL\", data[0]) #å°†é“¾æ¥æ›¿æ¢ä¸ºURL \n",
    "                data[0] = re.sub('@\\w+|\\n|[%s]+' %punc,\"\", data[0])\n",
    "                data[0] = bodyNorm(data[0]) #å»è¡¨æƒ…\n",
    "                if len(data[0]) > 1:\n",
    "                    f.write(data[0].strip() + '\\t' + data[1] + '\\n')\n",
    "    elif islabel == 1 and train == 'test':\n",
    "        with open(data_path, 'a',encoding='UTF-8') as f:\n",
    "            for data in  all_data: \n",
    "                count += 1\n",
    "                if count < int(0.9 * len(all_data)):#è®­ç»ƒé›†å’Œæµ‹è¯•é›†9:1\n",
    "                    continue\n",
    "               \n",
    "                #print(type(data))\n",
    "                data = [str(i) for i in data]\n",
    "                #data[0] = re.sub('@\\w+|http.*\\w+|\\n|[%s]+' %punc,\"\", data[0])\n",
    "                data[0] = re.sub('http.*',\" URL\", data[0]) #å°†é“¾æ¥æ›¿æ¢ä¸ºURL \n",
    "                data[0] = re.sub('@\\w+|\\n|[%s]+' %punc,\"\", data[0])\n",
    "                data[0] = bodyNorm(data[0]) #å»è¡¨æƒ…\n",
    "                if len(data[0]) > 1:\n",
    "                    f.write(data[0].strip() + '\\t' + data[1] + '\\n')\n",
    "   \n",
    "    if islabel == 0 and train == 'train':\n",
    "        with open(data_path, 'a',encoding='UTF-8') as f:\n",
    "            for data in  all_data:\n",
    "                if count > int(0.9 * len(all_data)):#è®­ç»ƒé›†å’Œæµ‹è¯•é›†9:1\n",
    "                    break\n",
    "                count += 1\n",
    "                #print(type(data))\n",
    "                data = [str(i) for i in data]\n",
    "                #data[0] = re.sub('@\\w+|http.*\\w+|\\n|[%s]+' %punc,\"\", data[0])\n",
    "                data[0] = re.sub('http.*',\" URL\", data[0]) #å°†é“¾æ¥æ›¿æ¢ä¸ºURL \n",
    "                data[0] = re.sub('@\\w+|\\n|[%s]+' %punc,\"\", data[0])\n",
    "                data[0] = bodyNorm(data[0]) #å»è¡¨æƒ…\n",
    "                if len(data[0]) > 1 != \"\":\n",
    "                    f.write(data[0].strip() + '\\n')\n",
    "    elif islabel == 0 and train == 'test':\n",
    "        with open(data_path, 'a',encoding='UTF-8') as f:\n",
    "            for data in  all_data:\n",
    "                count += 1\n",
    "                if count < int(0.9 * len(all_data)):#è®­ç»ƒé›†å’Œæµ‹è¯•é›†9:1\n",
    "                    continue\n",
    "                \n",
    "                #print(type(data))\n",
    "                data = [str(i) for i in data]\n",
    "                #data[0] = re.sub('@\\w+|http.*\\w+|\\n|[%s]+' %punc,\"\", data[0])\n",
    "                data[0] = re.sub('http.*',\" URL\", data[0]) #å°†é“¾æ¥æ›¿æ¢ä¸ºURL \n",
    "                data[0] = re.sub('@\\w+|\\n|[%s]+' %punc,\"\", data[0])\n",
    "                data[0] = bodyNorm(data[0]) #å»è¡¨æƒ…\n",
    "                if len(data[0]) > 1:\n",
    "                    f.write(data[0].strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_save(\"train\", all_data, 1, \"data.txt\")\n",
    "all_data_save(\"test\", all_data, 1, \"data.txt\")\n",
    "all_data_save(\"train\", all_data, 0, \"data_non_label.txt\")\n",
    "all_data_save(\"test\", all_data, 0, \"data_non_label.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆ†è¯å’Œå»åœç”¨è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ†è¯å’Œå»åœç”¨è¯\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from string import punctuation as enpunctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences: 5222'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/train_data_non_label.txt', 'r', encoding='utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    # stæ˜¯sentenceçš„ç¼©å†™\n",
    "    #srctweet = [st.split() for st in lines]\n",
    "    #ä½¿ç”¨nltkåˆ†è¯\n",
    "    punctuations = set([str(i) for i in enpunctuation]) | set([str(i) for i in zhonPunctuation]) #å»æ ‡ç‚¹ç¬¦å·\n",
    "    srctweet = [nltk.word_tokenize(st) for st in lines if st != '' and st not in punctuations]\n",
    "    #srctweet = [nltk.word_tokenize(st) for st in lines]\n",
    "\n",
    "'# sentences: %d' % len(srctweet) # è¾“å‡º '# sentences: 42068'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: 15 ['LATEST', 'UPDATE', 'Attacker', 'shot', 'dead']\n",
      "# tokens: 19 ['I', 'want', 'to', 'commend', 'the']\n",
      "# tokens: 16 ['Left', 'police', 'that', 'Obama', 'condemned']\n"
     ]
    }
   ],
   "source": [
    "for st in srctweet[:3]:\n",
    "    print('# tokens:', len(st), st[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ˜³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ğŸ˜’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ğŸ™ˆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ğŸ˜³ğŸ™ˆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ğŸ˜¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stopword\n",
       "0        ğŸ˜³\n",
       "1        ğŸ˜’\n",
       "2        ğŸ™ˆ\n",
       "3       ğŸ˜³ğŸ™ˆ\n",
       "4        ğŸ˜¬"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=pd.read_csv(\"./data/stop_words-master/english.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ¸…é™¤æ ‡ç‚¹ç¬¦å·\n",
    "def del_mark(word):\n",
    "    punc = '~`!#$%^&*()_+-=|\\';\":/.,?><~Â·ï¼@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰â€”â€”+-=â€œï¼šâ€™ï¼›ã€ã€‚ï¼Œï¼Ÿã€‹ã€Š{\\]}\\[âœ'\n",
    "    return re.sub(r\"[%s]+\" %punc, \"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stopwords(srctweet,stopwords):\n",
    "    srctweet_clean = []\n",
    "    all_words = []\n",
    "    for line in srctweet:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            word = del_mark(word)\n",
    "            line_clean.append(word)\n",
    "            all_words.append(word)##è®°å½•æ‰€æœ‰line_cleanä¸­çš„è¯\n",
    "        srctweet_clean.append(line_clean)\n",
    "    return srctweet_clean,all_words\n",
    "    #print (contents_clean)\n",
    "        \n",
    "\n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "srctweet_clean,all_words = drop_stopwords(srctweet,stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LATEST', 'UPDATE', 'Attacker', 'shot', 'dead', 'Parliament', 'soldier', 'shot', 'National', 'War', 'Memorial', 'URL']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 'LATEST', 46311, 5222)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(srctweet_clean[0]), all_words[0], len(all_words), len(srctweet_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: 12 ['LATEST', 'UPDATE', 'Attacker', 'shot', 'dead']\n",
      "# tokens: 6 ['commend', 'courage', 'bravery', 'responders', 'ground']\n",
      "# tokens: 13 ['Left', 'police', 'Obama', 'condemned', 'Yanukovych']\n"
     ]
    }
   ],
   "source": [
    "for st in srctweet_clean[:3]:\n",
    "    print('# tokens:', len(st), st[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "max_srctweet = 0 #æœ€å¤§è¯„è®º\n",
    "for i in srctweet_clean:\n",
    "    max_srctweet = max(max_srctweet, len(i))\n",
    "print(max_srctweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å°†all_wordsä¿å­˜åˆ°txt\n",
    "def all_words_save_to_txt(all_words, path):\n",
    "    # åœ¨ç”Ÿæˆall_data.txtä¹‹å‰ï¼Œé¦–å…ˆå°†å…¶æ¸…ç©º\n",
    "    with open(path, 'w') as f:\n",
    "        f.seek(0)\n",
    "        f.truncate()\n",
    "\n",
    "    with open(path, 'a',encoding='UTF-8') as f:\n",
    "        for data in  all_words:\n",
    "                f.write(data+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_path = './data/all_words.txt'\n",
    "all_words_save_to_txt(all_words, all_words_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å»ºç«‹ç´¢å¼•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 39353'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = collections.Counter([tk for st in srctweet_clean for tk in st])\n",
    "counter = dict(filter(lambda x: x[1] >= 3, counter.items())) #å°†å‡ºç°æ¬¡æ•°å°äº3çš„è¯å»æ‰\n",
    "\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "           for st in srctweet]\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "'# tokens: %d' % num_tokens # è¾“å‡º '# tokens: '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# äºŒæ¬¡é‡‡æ ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 14434'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "'# tokens: %d' % sum([len(st) for st in subsampled_dataset]) # '# tokens: '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æå–ä¸­å¿ƒè¯å’ŒèƒŒæ™¯è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:  # æ¯ä¸ªå¥å­è‡³å°‘è¦æœ‰2ä¸ªè¯æ‰å¯èƒ½ç»„æˆä¸€å¯¹â€œä¸­å¿ƒè¯-èƒŒæ™¯è¯â€\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i)  # å°†ä¸­å¿ƒè¯æ’é™¤åœ¨èƒŒæ™¯è¯ä¹‹å¤–\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [1, 3]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [2, 3, 5, 6]\n",
      "center 5 has contexts [3, 4, 6]\n",
      "center 6 has contexts [5]\n",
      "center 7 has contexts [8]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [7, 8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è´Ÿé‡‡æ ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                # æ ¹æ®æ¯ä¸ªè¯çš„æƒé‡ï¼ˆsampling_weightsï¼‰éšæœºç”Ÿæˆkä¸ªè¯çš„ç´¢å¼•ä½œä¸ºå™ªå£°è¯ã€‚\n",
    "                # ä¸ºäº†é«˜æ•ˆè®¡ç®—ï¼Œå¯ä»¥å°†kè®¾å¾—ç¨å¤§ä¸€ç‚¹\n",
    "                i, neg_candidates = 0, random.choices(\n",
    "                    population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            # å™ªå£°è¯ä¸èƒ½æ˜¯èƒŒæ™¯è¯\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    \"\"\"ç”¨ä½œDataLoaderçš„å‚æ•°collate_fn: è¾“å…¥æ˜¯ä¸ªé•¿ä¸ºbatchsizeçš„list, \n",
    "    listä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯Datasetç±»è°ƒç”¨__getitem__å¾—åˆ°çš„ç»“æœ\n",
    "    \"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 42])\n",
      "masks shape: torch.Size([512, 42])\n",
      "labels shape: torch.Size([512, 42])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "dataset = MyDataset(all_centers, \n",
    "                    all_contexts, \n",
    "                    all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                            collate_fn=batchify, \n",
    "                            num_workers=num_workers)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
    "                           'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 6.5193e-01,  6.1295e-01, -1.4379e+00, -1.0193e-01],\n",
       "        [ 2.9354e-01,  1.1260e+00,  1.6213e+00, -2.0923e+00],\n",
       "        [ 1.0044e+00,  9.2871e-02,  2.2227e-03, -1.3675e+00],\n",
       "        [ 9.4186e-01,  9.1879e-01,  5.1415e-01, -1.1744e+00],\n",
       "        [ 5.7575e-01, -4.8407e-01, -2.0681e-01,  6.2633e-01],\n",
       "        [-7.9889e-01, -4.9089e-01,  3.4041e-02,  1.9713e+00],\n",
       "        [-1.3460e+00,  1.0711e+00,  3.3568e-02, -1.8251e+00],\n",
       "        [ 7.6171e-01, -3.1890e-01, -9.7253e-01,  1.0883e+00],\n",
       "        [-1.0393e+00,  5.8319e-02, -8.5887e-01,  4.9098e-01],\n",
       "        [-6.7741e-01, -1.8605e+00,  1.3469e+00,  2.3311e+00],\n",
       "        [-5.5728e-01, -2.9765e-01,  2.2655e-01, -1.8627e+00],\n",
       "        [ 3.2342e-02,  3.7914e-01, -1.1053e-01, -3.8136e-01],\n",
       "        [-1.5619e+00, -1.5674e+00,  1.7292e+00, -1.6584e+00],\n",
       "        [ 3.3953e-01, -1.0159e+00,  2.1279e+00,  2.8211e+00],\n",
       "        [-1.0350e+00,  1.2081e+00, -1.3615e+00,  3.8770e-01],\n",
       "        [ 7.1671e-02, -7.9976e-01,  2.8926e-01, -1.1335e+00],\n",
       "        [-1.7214e+00,  3.6336e-01,  1.2801e+00, -3.3622e-01],\n",
       "        [ 1.6669e+00, -1.1706e+00, -5.7033e-01, -1.1929e+00],\n",
       "        [-3.2427e-01,  1.3381e+00, -4.2235e-01,  9.9057e-01],\n",
       "        [ 2.1748e-01,  9.4328e-01,  1.0309e+00, -1.0847e+00]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
    "embed.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2935,  1.1260,  1.6213, -2.0923],\n",
       "         [ 1.0044,  0.0929,  0.0022, -1.3675],\n",
       "         [ 0.9419,  0.9188,  0.5141, -1.1744]],\n",
       "\n",
       "        [[ 0.5758, -0.4841, -0.2068,  0.6263],\n",
       "         [-0.7989, -0.4909,  0.0340,  1.9713],\n",
       "         [-1.3460,  1.0711,  0.0336, -1.8251]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)\n",
    "embed(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å°æ‰¹é‡ä¹˜æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 6])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((2, 1, 4))\n",
    "Y = torch.ones((2, 4, 6))\n",
    "torch.bmm(X, Y).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è·³å­—æ¨¡å‹å‘å‰è®¡ç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# äº¤å‰ç†µæŸå¤±å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self): # none mean sum\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        \"\"\"\n",
    "        input â€“ Tensor shape: (batch_size, len)\n",
    "        target â€“ Tensor of the same shape as input\n",
    "        \"\"\"\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        return res.mean(dim=1)\n",
    "\n",
    "loss = SigmoidBinaryCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 1.2100])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
    "# æ ‡ç­¾å˜é‡labelä¸­çš„1å’Œ0åˆ†åˆ«ä»£è¡¨èƒŒæ™¯è¯å’Œå™ªå£°è¯\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]])\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])  # æ©ç å˜é‡\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.float().sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740\n",
      "1.2100\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "    return - math.log(1 / (1 + math.exp(-x)))\n",
    "\n",
    "print('%.4f' % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) / 4)) # æ³¨æ„1-sigmoid(x) = sigmoid(-x)\n",
    "print('%.4f' % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆå§‹åŒ–æ¨¡å‹å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å®šä¹‰è®­ç»ƒå‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "\n",
    "            # ä½¿ç”¨æ©ç å˜é‡maskæ¥é¿å…å¡«å……é¡¹å¯¹æŸå¤±å‡½æ•°è®¡ç®—çš„å½±å“\n",
    "            l = (loss(pred.view(label.shape), label, mask) *\n",
    "                 mask.shape[1] / mask.float().sum(dim=1)).mean() # ä¸€ä¸ªbatchçš„å¹³å‡loss\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n",
      "epoch 1, loss 0.04, time 0.32s\n",
      "epoch 2, loss 0.03, time 0.33s\n",
      "epoch 3, loss 0.03, time 0.28s\n",
      "epoch 4, loss 0.03, time 0.30s\n",
      "epoch 5, loss 0.03, time 0.28s\n",
      "epoch 6, loss 0.03, time 0.28s\n",
      "epoch 7, loss 0.03, time 0.28s\n",
      "epoch 8, loss 0.03, time 0.32s\n",
      "epoch 9, loss 0.03, time 0.28s\n",
      "epoch 10, loss 0.03, time 0.28s\n"
     ]
    }
   ],
   "source": [
    "#å¼€å§‹è®­ç»ƒ\n",
    "train(net, 0.008, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happeningè¯åµŒå…¥: torch.Size([100]) \n",
      "\n",
      "cosine sim=0.404: confirmed\n",
      "cosine sim=0.358: Sky\n",
      "cosine sim=0.339: St\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    print(query_token + \"è¯åµŒå…¥:\", x.shape, \"\\n\")\n",
    "    # æ·»åŠ çš„1e-9æ˜¯ä¸ºäº†æ•°å€¼ç¨³å®šæ€§\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:  # é™¤å»è¾“å…¥è¯\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('happening', 3, net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {\n",
    "    'en':'è‹±è¯­',\n",
    "    'cn':'ä¸­æ–‡',\n",
    "    'fr':'æ³•è¯­',\n",
    "    'jp':'æ—¥è¯­'\n",
    "}\n",
    "l1 = list(d1.items())\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LATEST', 0)\n"
     ]
    }
   ],
   "source": [
    "#å°†token_to_idxä¿å­˜åœ¨token_to_idx.txt\n",
    "token_to_idx_list = list(token_to_idx.items()) #å°†è¯å…¸è½¬æ¢ä¸ºlist\n",
    "print(token_to_idx_list[0])\n",
    "with open(\"./data/token_to_idx.txt\", 'w') as f:\n",
    "    f.seek(0)\n",
    "    f.truncate()\n",
    "with open(\"./data/token_to_idx.txt\", 'a', encoding='utf8') as f:\n",
    "    for i in token_to_idx_list:\n",
    "        f.write(i[0] + \"\\t\" + str(i[1]) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜æ¨¡å‹embedding\n",
    "torch.save(net, \"./data/net.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net1 = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size)\n",
    ")\n",
    "net1 = torch.load(\"./data/net.pt\")\n",
    "net1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_tensor in net[0].state_dict():\n",
    "    print(param_tensor, \n",
    "          \"\\t\", \n",
    "          net[0].state_dict()[param_tensor].size()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net[0].weight.data[token_to_idx[\"Germanwings\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(net[0].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"net_params.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.load('net_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
