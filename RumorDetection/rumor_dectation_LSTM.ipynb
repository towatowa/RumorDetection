{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ËØªÂèñÊï∞ÊçÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import sys\n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "def loadData(filename):\n",
    "    data = []\n",
    "    fr = open(filename, 'r', encoding='utf8')\n",
    "    for line in fr.readlines():                 #ÈÄêË°åËØªÂèñ\n",
    "        lineArr = line.strip().split('\\t')      #Êª§Èô§Ë°åÈ¶ñË°åÂ∞æÁ©∫Ê†ºÔºå‰ª•\\t‰Ωú‰∏∫ÂàÜÈöîÁ¨¶ÔºåÂØπËøôË°åËøõË°åÂàÜËß£\n",
    "        num = np.shape(lineArr)[0]     \n",
    "        data.append([\"\".join(lineArr[0:num-1]), int(lineArr[num-1])])#Ëøô‰∏ÄË°åÁöÑÈô§ÊúÄÂêé‰∏Ä‰∏™Ë¢´Ê∑ªÂä†‰∏∫Êï∞ÊçÆ\n",
    "        #labelMat.append(int(lineArr[num-1]))#Ëøô‰∏ÄË°åÁöÑÊúÄÂêé‰∏Ä‰∏™Êï∞ÊçÆË¢´Ê∑ªÂä†‰∏∫Ê†áÁ≠æ\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charlie Hebdos Last Tweet Spoofed ISIS Leader AlBaghdadi  URL 0\n",
      "Trocadero square in Paris evacuated Policemen pointing their gun URL 1\n",
      "held by gunman at kosher supermarket in Paris as nd hostagetaking underway AP Gunman linked to Thursdays killing of policewoman 0\n"
     ]
    }
   ],
   "source": [
    "train_data = loadData(\"./data/train_data.txt\")\n",
    "test_data = loadData(\"./data/test_data.txt\")\n",
    "\n",
    "\"test txt len: \", len(test_data), \"train txt len: \", len(train_data)\n",
    "for i in train_data[0:3]:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÂéªÂÅúÁî®ËØç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÂàÜËØçÂíåÂéªÂÅúÁî®ËØç\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from string import punctuation as enpunctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhonPunctuation = u'''ÔºÇ ÔºÉ ÔºÑ ÔºÖ ÔºÜ Ôºá Ôºà Ôºâ Ôºä Ôºã Ôºå Ôºç Ôºè Ôºö Ôºõ Ôºú Ôºù Ôºû Ôº† Ôºª Ôºº ÔºΩ Ôºæ Ôºø ÔΩÄ ÔΩõ ÔΩú ÔΩù ÔΩû ÔΩü ÔΩ† ÔΩ¢ ÔΩ£ ÔΩ§  „ÄÉ „Äà „Äâ „Ää „Äã „Äå „Äç „Äé „Äè „Äê „Äë „Äî „Äï „Äñ „Äó „Äò „Äô „Äö „Äõ „Äú „Äù „Äû „Äü  „Äæ „Äø ‚Äì ‚Äî ‚Äò ‚Äô ‚Äõ ‚Äú ‚Äù ‚Äû ‚Äü ‚Ä¶ ‚Äß Ôπè Ôπë Ôπî ¬∑ ÔºÅ Ôºü ÔΩ° ‚Üí „ÄÅ „ÄÇ'''\n",
    "punctuations = set([str(i) for i in enpunctuation]) | set([str(i) for i in zhonPunctuation]) #ÂéªÊ†áÁÇπÁ¨¶Âè∑\n",
    "srctweet1 = [nltk.word_tokenize(st[0]) for st in train_data if st[0] != '' and st[0] not in punctuations]\n",
    "srctweet2 = [nltk.word_tokenize(st[0]) for st in test_data if st[0] != '' and st[0] not in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üò≥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üòí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üôà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üò≥üôà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üò¨</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stopword\n",
       "0        üò≥\n",
       "1        üòí\n",
       "2        üôà\n",
       "3       üò≥üôà\n",
       "4        üò¨"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=pd.read_csv(\"./data/stop_words-master/english.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ê∏ÖÈô§Ê†áÁÇπÁ¨¶Âè∑\n",
    "def del_mark(word):\n",
    "    punc = '~`!#$%^&*()_+-=|\\';\":/.,?><~¬∑ÔºÅ@#Ôø•%‚Ä¶‚Ä¶&*ÔºàÔºâ‚Äî‚Äî+-=‚ÄúÔºö‚ÄôÔºõ„ÄÅ„ÄÇÔºåÔºü„Äã„Ää{\\]}\\[‚úè'\n",
    "    return re.sub(r\"[%s]+\" %punc, \"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, ['Trocadero square Paris evacuated Policemen gun URL', 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_stopwords(srctweet,stopwords):\n",
    "    srctweet_clean = []\n",
    "    all_words = []\n",
    "    for line in srctweet:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            word = del_mark(word)\n",
    "            line_clean.append(word)\n",
    "            all_words.append(word)##ËÆ∞ÂΩïÊâÄÊúâline_clean‰∏≠ÁöÑËØç\n",
    "        srctweet_clean.append(line_clean)\n",
    "    return srctweet_clean,all_words\n",
    "    #print (contents_clean)\n",
    "        \n",
    "\n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "srctweet1,all_words = drop_stopwords(srctweet1,stopwords)\n",
    "srctweet2,_  = drop_stopwords(srctweet2,stopwords)\n",
    "i=0\n",
    "for line in srctweet1:\n",
    "    train_data[i][0] = \" \".join(line)\n",
    "    i+=1\n",
    "i=0\n",
    "for line in srctweet2:\n",
    "    test_data[i][0] = \" \".join(line)\n",
    "    i+=1\n",
    "type(train_data[0]), train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_imdb(data):\n",
    "    \"\"\"\n",
    "    data: list of [string, label]\n",
    "    \"\"\"\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    return [tokenizer(review) for review, _ in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåÊàë‰ª¨ÂèØ‰ª•Ê†πÊçÆÂàÜÂ•ΩËØçÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÊù•ÂàõÂª∫ËØçÂÖ∏‰∫Ü„ÄÇÊàë‰ª¨Âú®ËøôÈáåËøáÊª§Êéâ‰∫ÜÂá∫Áé∞Ê¨°Êï∞Â∞ë‰∫é5ÁöÑËØç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('# words in vocab:', 513)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab_imdb(data):\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "'# words in vocab:', len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Âõ†‰∏∫ÊØèÊù°ËØÑËÆ∫ÈïøÂ∫¶‰∏ç‰∏ÄËá¥ÊâÄ‰ª•‰∏çËÉΩÁõ¥Êé•ÁªÑÂêàÊàêÂ∞èÊâπÈáèÔºåÊàë‰ª¨ÂÆö‰πâpreprocess_imdbÂáΩÊï∞ÂØπÊØèÊù°ËØÑËÆ∫ËøõË°åÂàÜËØçÔºåÂπ∂ÈÄöËøáËØçÂÖ∏ËΩ¨Êç¢ÊàêËØçÁ¥¢ÂºïÔºåÁÑ∂ÂêéÈÄöËøáÊà™Êñ≠ÊàñËÄÖË°•0Êù•Â∞ÜÊØèÊù°ËØÑËÆ∫ÈïøÂ∫¶Âõ∫ÂÆöÊàê300„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb(data, vocab):\n",
    "    max_l = 300  # Â∞ÜÊØèÊù°ËØÑËÆ∫ÈÄöËøáÊà™Êñ≠ÊàñËÄÖË°•0Ôºå‰ΩøÂæóÈïøÂ∫¶ÂèòÊàê300\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÂàõÂª∫Êï∞ÊçÆËø≠‰ª£Âô®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåÊàë‰ª¨ÂàõÂª∫Êï∞ÊçÆËø≠‰ª£Âô®„ÄÇÊØèÊ¨°Ëø≠‰ª£Â∞ÜËøîÂõû‰∏Ä‰∏™Â∞èÊâπÈáèÁöÑÊï∞ÊçÆ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÊâìÂç∞Á¨¨‰∏Ä‰∏™Â∞èÊâπÈáèÊï∞ÊçÆÁöÑÂΩ¢Áä∂‰ª•ÂèäËÆ≠ÁªÉÈõÜ‰∏≠Â∞èÊâπÈáèÁöÑ‰∏™Êï∞„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 300]) y torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches:', 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‰ΩøÁî®Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÊ®°Âûã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Âú®Ëøô‰∏™Ê®°Âûã‰∏≠ÔºåÊØè‰∏™ËØçÂÖàÈÄöËøáÂµåÂÖ•Â±ÇÂæóÂà∞ÁâπÂæÅÂêëÈáè„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨‰ΩøÁî®ÂèåÂêëÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÂØπÁâπÂæÅÂ∫èÂàóËøõ‰∏ÄÊ≠•ÁºñÁ†ÅÂæóÂà∞Â∫èÂàó‰ø°ÊÅØ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Â∞ÜÁºñÁ†ÅÁöÑÂ∫èÂàó‰ø°ÊÅØÈÄöËøáÂÖ®ËøûÊé•Â±ÇÂèòÊç¢‰∏∫ËæìÂá∫„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÂèØ‰ª•Â∞ÜÂèåÂêëÈïøÁü≠ÊúüËÆ∞ÂøÜÂú®ÊúÄÂàùÊó∂Èó¥Ê≠•ÂíåÊúÄÁªàÊó∂Èó¥Ê≠•ÁöÑÈöêËóèÁä∂ÊÄÅËøûÁªìÔºå‰Ωú‰∏∫ÁâπÂæÅÂ∫èÂàóÁöÑË°®ÂæÅ‰º†ÈÄíÁªôËæìÂá∫Â±ÇÂàÜÁ±ª„ÄÇÂú®‰∏ãÈù¢ÂÆûÁé∞ÁöÑBiRNNÁ±ª‰∏≠ÔºåEmbeddingÂÆû‰æãÂç≥ÂµåÂÖ•Â±ÇÔºåLSTMÂÆû‰æãÂç≥‰∏∫Â∫èÂàóÁºñÁ†ÅÁöÑÈöêËóèÂ±ÇÔºåLinearÂÆû‰æãÂç≥ÁîüÊàêÂàÜÁ±ªÁªìÊûúÁöÑËæìÂá∫Â±Ç„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # bidirectionalËÆæ‰∏∫TrueÂç≥ÂæóÂà∞ÂèåÂêëÂæ™ÁéØÁ•ûÁªèÁΩëÁªú\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, \n",
    "                                hidden_size=num_hiddens, \n",
    "                                num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        # ÂàùÂßãÊó∂Èó¥Ê≠•ÂíåÊúÄÁªàÊó∂Èó¥Ê≠•ÁöÑÈöêËóèÁä∂ÊÄÅ‰Ωú‰∏∫ÂÖ®ËøûÊé•Â±ÇËæìÂÖ•\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputsÁöÑÂΩ¢Áä∂ÊòØ(ÊâπÈáèÂ§ßÂ∞è, ËØçÊï∞)ÔºåÂõ†‰∏∫LSTMÈúÄË¶ÅÂ∞ÜÂ∫èÂàóÈïøÂ∫¶(seq_len)‰Ωú‰∏∫Á¨¨‰∏ÄÁª¥ÔºåÊâÄ‰ª•Â∞ÜËæìÂÖ•ËΩ¨ÁΩÆÂêé\n",
    "        # ÂÜçÊèêÂèñËØçÁâπÂæÅÔºåËæìÂá∫ÂΩ¢Áä∂‰∏∫(ËØçÊï∞, ÊâπÈáèÂ§ßÂ∞è, ËØçÂêëÈáèÁª¥Â∫¶)\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        # rnn.LSTMÂè™‰º†ÂÖ•ËæìÂÖ•embeddingsÔºåÂõ†Ê≠§Âè™ËøîÂõûÊúÄÂêé‰∏ÄÂ±ÇÁöÑÈöêËóèÂ±ÇÂú®ÂêÑÊó∂Èó¥Ê≠•ÁöÑÈöêËóèÁä∂ÊÄÅ„ÄÇ\n",
    "        # outputsÂΩ¢Áä∂ÊòØ(ËØçÊï∞, ÊâπÈáèÂ§ßÂ∞è, 2 * ÈöêËóèÂçïÂÖÉ‰∏™Êï∞)\n",
    "        outputs, _ = self.encoder(embeddings) # output, (h, c)\n",
    "        # ËøûÁªìÂàùÂßãÊó∂Èó¥Ê≠•ÂíåÊúÄÁªàÊó∂Èó¥Ê≠•ÁöÑÈöêËóèÁä∂ÊÄÅ‰Ωú‰∏∫ÂÖ®ËøûÊé•Â±ÇËæìÂÖ•„ÄÇÂÆÉÁöÑÂΩ¢Áä∂‰∏∫\n",
    "        # (ÊâπÈáèÂ§ßÂ∞è, 4 * ÈöêËóèÂçïÂÖÉ‰∏™Êï∞)„ÄÇ\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÂàõÂª∫‰∏Ä‰∏™Âê´‰∏§‰∏™ÈöêËóèÂ±ÇÁöÑÂèåÂêëÂæ™ÁéØÁ•ûÁªèÁΩëÁªú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ËØªÂèñËØçÂµåÂÖ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_idx_list = loadData(\"./data/token_to_idx.txt\")\n",
    "token_to_idx = {}\n",
    "for item in token_to_idx_list:\n",
    "    dic = {item[0]: item[1]}\n",
    "    token_to_idx.update(dic)\n",
    "\n",
    "token_to_idx['happening']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net1 = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(token_to_idx), embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(token_to_idx), embedding_dim=embed_size)\n",
    ")\n",
    "net1 = torch.load(\"./data/net.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2327, 100]), 2327)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1[0].weight.data.shape, len(token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 157 oov words.\n"
     ]
    }
   ],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"‰ªéÈ¢ÑËÆ≠ÁªÉÂ•ΩÁöÑvocab‰∏≠ÊèêÂèñÂá∫wordsÂØπÂ∫îÁöÑËØçÂêëÈáè\"\"\"\n",
    "    W = pretrained_vocab.weight.data\n",
    "    embed = torch.zeros(len(words),  W.shape[1]) # ÂàùÂßãÂåñ‰∏∫0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = token_to_idx[word]\n",
    "            embed[i, :] = W[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "net.embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.itos, net1[0]))\n",
    "net.embedding.weight.requires_grad = False # Áõ¥Êé•Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÂ•ΩÁöÑ, ÊâÄ‰ª•‰∏çÈúÄË¶ÅÊõ¥Êñ∞ÂÆÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ËÆ≠ÁªÉÂπ∂ËØÑ‰ª∑Ê®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.2419, train acc 0.903, test acc 0.847, time 1.9 sec\n",
      "epoch 2, loss 0.0853, train acc 0.943, test acc 0.856, time 1.8 sec\n",
      "epoch 3, loss 0.0426, train acc 0.955, test acc 0.856, time 1.8 sec\n",
      "epoch 4, loss 0.0245, train acc 0.970, test acc 0.856, time 1.8 sec\n",
      "epoch 5, loss 0.0195, train acc 0.973, test acc 0.852, time 1.8 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 10\n",
    "# Ë¶ÅËøáÊª§Êéâ‰∏çËÆ°ÁÆóÊ¢ØÂ∫¶ÁöÑembeddingÂèÇÊï∞\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './data/LSTM_net_loss=0.0135_acc=0.876.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    \"\"\"sentenceÊòØËØçËØ≠ÁöÑÂàóË°®\"\"\"\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    #return 'positive' if label.item() == 1 else 'negative'\n",
    "    return int(1) if label.item() == 1 else int(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÊµãËØïÊï∞ÊçÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, int)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data[0][0]), type(test_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÂàÜËØçÂíåÂéªÂÅúÁî®ËØç\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from string import punctuation as enpunctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhonPunctuation = u'''ÔºÇ ÔºÉ ÔºÑ ÔºÖ ÔºÜ Ôºá Ôºà Ôºâ Ôºä Ôºã Ôºå Ôºç Ôºè Ôºö Ôºõ Ôºú Ôºù Ôºû Ôº† Ôºª Ôºº ÔºΩ Ôºæ Ôºø ÔΩÄ ÔΩõ ÔΩú ÔΩù ÔΩû ÔΩü ÔΩ† ÔΩ¢ ÔΩ£ ÔΩ§  „ÄÉ „Äà „Äâ „Ää „Äã „Äå „Äç „Äé „Äè „Äê „Äë „Äî „Äï „Äñ „Äó „Äò „Äô „Äö „Äõ „Äú „Äù „Äû „Äü  „Äæ „Äø ‚Äì ‚Äî ‚Äò ‚Äô ‚Äõ ‚Äú ‚Äù ‚Äû ‚Äü ‚Ä¶ ‚Äß Ôπè Ôπë Ôπî ¬∑ ÔºÅ Ôºü ÔΩ° ‚Üí „ÄÅ „ÄÇ'''\n",
    "punctuations = set([str(i) for i in enpunctuation]) | set([str(i) for i in zhonPunctuation]) #ÂéªÊ†áÁÇπÁ¨¶Âè∑\n",
    "srctweet = [nltk.word_tokenize(st[0]) for st in test_data if st[0] != '' and st[0] not in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üò≥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üòí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üôà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üò≥üôà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üò¨</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stopword\n",
       "0        üò≥\n",
       "1        üòí\n",
       "2        üôà\n",
       "3       üò≥üôà\n",
       "4        üò¨"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=pd.read_csv(\"./data/stop_words-master/english.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ê∏ÖÈô§Ê†áÁÇπÁ¨¶Âè∑\n",
    "def del_mark(word):\n",
    "    punc = '~`!#$%^&*()_+-=|\\';\":/.,?><~¬∑ÔºÅ@#Ôø•%‚Ä¶‚Ä¶&*ÔºàÔºâ‚Äî‚Äî+-=‚ÄúÔºö‚ÄôÔºõ„ÄÅ„ÄÇÔºåÔºü„Äã„Ää{\\]}\\[‚úè'\n",
    "    return re.sub(r\"[%s]+\" %punc, \"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'stopword'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-afe5981bce7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0msrctweet_clean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrop_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrctweet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrctweet_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrctweet_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'stopword'"
     ]
    }
   ],
   "source": [
    "def drop_stopwords(srctweet,stopwords):\n",
    "    srctweet_clean = []\n",
    "    all_words = []\n",
    "    for line in srctweet:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            word = del_mark(word)\n",
    "            line_clean.append(word)\n",
    "            all_words.append(word)##ËÆ∞ÂΩïÊâÄÊúâline_clean‰∏≠ÁöÑËØç\n",
    "        srctweet_clean.append(line_clean)\n",
    "    return srctweet_clean,all_words\n",
    "    #print (contents_clean)\n",
    "        \n",
    "\n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "srctweet_clean,all_words = drop_stopwords(srctweet,stopwords)\n",
    "type(srctweet_clean[0]), srctweet_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = torch.load(r\"D:\\NoteBook\\RumerDetection\\data\\LSTM_net_loss=0.0023_acc=0.856.pt\")\n",
    "net2.embedding.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = []\n",
    "for item in test_data:\n",
    "    test_result.append([item, predict_sentiment(net2, vocab, item[0].split())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_result[0]), type(test_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ê≠£Á°ÆÊï∞:', 163, 'ÊÄªÊï∞: ', 209, 'Ê≠£Á°ÆÁéáÔºö', 77.99043062200957)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(0, len(test_result)):\n",
    "    if int(test_result[i][1]) ^ test_data[i][1] == 0:\n",
    "        count += 1\n",
    "'Ê≠£Á°ÆÊï∞:', count, 'ÊÄªÊï∞: ', len(test_result), 'Ê≠£Á°ÆÁéáÔºö', 1.0 * count/len(test_result) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './data/LSTM_net_loss=0.0135_acc=0.876.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data:  ['UPDATE', 'Reports', 'Sydney', 'Opera', 'House', 'evacuated', 'News'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Sydney', 'siege', 'police', 'enter', 'cafe', 'hostages', 'flee', 'scene', 'gunfire', 'heard', 'reports', 'injured', 'person', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Map', 'locating', 'Paris', 'offices', 'satirical', 'magazine', 'Charlie', 'Hebdo', 'AFP', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['At', 'hostages', 'flee', 'lindtcafe', 'Sydney', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Plane', 'crashes', 'southern', 'France', 'board', 'Germanwings', 'budget', 'airline', 'flying', 'Barcelona', 'Dusseldorf', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['COMING', 'UP', 'LIVE', 'Ottawa', 'shooting', 'Stephen', 'Harper', 'nation', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['NHL', 'postpones', 'Wednesdays', 'LeafsSenators', 'tragedy', 'Ottawa', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Police', 'entered', 'Sydney', 'cafe', 'loud', 'bangs', 'flashes', 'stream', 'sydneysiege'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['GermanWings', 'copilot', 'fresh', 'Muslim', 'convert', 'URLConverts', 'Islam', 'disturbed', 'WhiteMuslims'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['sydneysiege', 'misnomer', 'Its', 'hostage', 'situation', 'Inside', 'cafe', 'You', 'siege', 'Gaza'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Recap', 'Parliament', 'Hill', 'lockdown', 'uniformed', 'Canadian', 'soldier', 'shot', 'War', 'Memorial', 'Suspect', 'loose', 'witnesses', 'rifle'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['MORE', 'Ottawa', 'shootings', 'suspect', 'identified', 'Michael', 'ZehafBibeau', 'unclear', 'shooters', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['How', 'Arab', 'media', 'responded', 'Charlie', 'Hebdo', 'attack', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Police', 'confirm', 'deaths', 'Frances', 'celebrated', 'cartoonists', 'Charb', 'Cabu', 'Wolinski', 'Tignous', 'CharlieHebdo', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['One', 'shooting', 'victim', 'succumbed', 'injuries', 'He', 'Canadian', 'Forces', 'Our', 'prayers', 'loved'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['CharlieHebdo', 'cartoonists', 'killed', 'Charb', 'Cabu', 'Tignous', 'Georges', 'WolinskiJeSuisCharlie', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'A', 'spokesperson', 'The', 'Ottawa', 'Hospital', 'confirms', 'patients', 'stable'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Alert', 'press', 'gallery', 'email', 'RCMP', 'advises', 'downtown', 'Ottawa', 'windows', 'roofs', 'ongoing', 'police', 'incident'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'We', 'reports', 'crash', 'Airbus', 'A', 'Germanwings', 'Barcelona', 'Dusseldorf'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Incidents', 'occurred', 'National', 'War', 'Memorial', 'Rideau', 'Centre', 'Parliament', 'Hill', 'morning', 'ottnews'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Andreas', 'Lubitz', 'copilot', 'Germanwings', 'flight', 'intentionally', 'commended', 'FAA', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['My', 'hostage', 'situation', 'Sydney', 'hurt'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['unprecedented', 'Paris', 'road', 'Porte', 'Vincennes', 'hostage', 'Armed', 'police', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['UPDATE', 'Reports', 'gunman', 'requesting', 'speak', 'Australian', 'leaders', 'radio', 'specialist', 'officers', 'contact', 'News'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['An', 'baby', 'hostages', 'Kosher', 'supermarket', 'Paris', 'Unbearably', 'sad', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['JUST', 'IN', 'women', 'Sydney', 'cafe', 'ongoing', 'hostage', 'situation', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Breaking', 'Shots', 'fired', 'northeast', 'Paris', 'believed', 'Charlie', 'Hebdo', 'suspects', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['RT', 'This', 'media', 'These', 'cleaning', 'riot', 'Ferguson', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Ferguson', 'cops', 'beat', 'charged', 'wproperty', 'damage', 'bleeding', 'uniforms', 'Then', 'lied', 'court', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Police', 'confirm', 'shootings', 'Ottawa', 'PM', 'safe', 'downtown', 'buildings', 'lockdown', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['The', 'correct', 'descriptor', 'shooter', 'radical', 'muslim', 'terrorist', 'ottnews', 'ottawashooting', 'cdnpoli'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['If', 'selfies', 'MartinPlace', 'sydneysiege'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'Germanwings', 'CEO', 'plane', 'minute', 'descent', 'crashing'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Boom', 'RT', 'Why', 'Ferguson', 'PD', 'suspect', 'hours', 'Michael', 'Brown', 'killed', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['NSW', 'Police', 'Police', 'urge', 'media', 'responsible', 'reporting', 'Speculation', 'unnecessary', 'alarm', 'SydneySiege'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Updated', 'An', 'armed', 'person', 'hostages', 'Sydney', 'cafe', 'displayed', 'black', 'flag', 'Arabic', 'script', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['RT', 'If', 'support', 'freedom', 'fighter', 'StopIslamists', 'FreedomOfSpeech', 'CharlieHebdo', 'RedNationRising', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'French', 'media', 'reporting', 'suspects', 'CharlieHebdo', 'attack', 'killed', 'More', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Military', 'helicopters', 'dropping', 'forces', 'fields', 'DammartinenGoele', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['This', 'photo', 'reported', 'suspected', 'Ottawa', 'shooter', 'Michael', 'Zehar', 'Bibeau', 'suspended', 'ISIS', 'Twitter', 'account', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['People', 'gather', 'silence', 'remember', 'Paris', 'shooting', 'victims', 'JeSuisCharlie', 'Photos', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Hostagetaker', 'kosher', 'market', 'Paris', 'killed', 'Mayor', 'confirms', 'brothers', 'killed', 'separate', 'hostagetaking', 'CharlieHebdo'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Banksy', 'reacts', 'CharlieHebdo', 'attack', 'poignant', 'drawing', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['MORE', 'Operation', 'detain', 'massacre', 'suspects', 'unfolding', 'DammartinenGoele', 'miles', 'northeast', 'Paris', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['DEVELOPING', 'Pilot', 'locked', 'cockpit', 'Alps', 'crash', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Canada', 'journalist', 'captures', 'outbreak', 'smallarms', 'Parliament', 'building', 'Ottawa', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Family', 'Michael', 'Brown', 'slams', 'Ferguson', 'police', 'chiefs', 'devious', 'blame', 'victim', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Five', 'escaped', 'Lindt', 'cafe', 'Two', 'female', 'hostages', 'moments', 'sydneysiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['President', 'Obamas', 'statement', 'CharlieHebdo', 'shooting', 'mention', 'freedom', 'speech', 'press'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['REMINDER', 'MikeBrown', 'kid', 'walk', 'street', 'And', 'HE', 'IS', 'DEAD', 'Those', 'Ferguson'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['French', 'official', 'attack', 'Paris', 'weekly', 'CharlieHebdo', 'Kalachnikov', 'automatic', 'rifles', 'rocket', 'launcher', 'Unprecedented', 'AFP'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['JUST', 'IN', 'Gunman', 'Sydney', 'cafe', 'siege', 'devices', 'city', 'Report', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['feels', 'weird', 'hearing', 'media', 'fountain', 'European', 'freedom', 'speech', 'CharlieHebdo', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['SYDNEY', 'Reports', 'hands', 'window', 'black', 'flag', 'Arabic', 'writing', 'MORE', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['PHOTOS', 'New', 'Yorkers', 'write', 'peaceful', 'protest', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Chairs', 'piled', 'door', 'This', 'surreal', 'RTgrahamctv', 'Shot', 'caucus', 'shooting', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'Gunman', 'SydneySiege', 'police', 'devices', 'city', 'demands', 'speak', 'Prime', 'Minister', 'acc', 'reports'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['You', 'offence', 'disapprove', 'silence', 'kill', 'We', 'intimidated', 'JeSuisCharlie', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['RIGHTNOW', 'bridges', 'Ottawa', 'Gatineau', 'NOW', 'CLOSED', 'Active', 'search', 'underway', 'est', 'ParliamentHill', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['agree', 'Ill', 'defend', 'death', 'Voltaire', 'CharlieHebdo', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Police', 'pursuit', 'multiple', 'shooters', 'Ottawa', 'PM', 'safe', 'downtown', 'buildings', 'lockdown', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['FRANCE', 'PHOTO', 'Ahmed', 'Merabet', 'French', 'Muslim', 'Cop', 'victim', 'CharlieHebdo', 'attack', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Can', 'terrorist', 'cunts', 'Paris', 'fucking', 'bullshit'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['The', 'person', 'killed', 'Charlie', 'Hebdo', 'attacks', 'Muslim', 'police', 'officer', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['At', 'gunman', 'SydneySiege', 'image', 'suspect', 'verified', 'No', 'injuries', 'police', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['support', 'cartoons', 'language', 'publishers', 'BUT', 'CharlieHebdo', 'attack', 'sickening', 'dead', 'terrorattack'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Police', 'thrown', 'crowd', 'Not', 'Protester', 'bullets', 'Ferguson'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Update', 'Telegraph', 'reporter', 'Henry', 'Samuels', 'tells', 'Sky', 'News', 'killed', 'shooting', 'Charlie', 'Hebdo', 'headquarters', 'Paris'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Killed', 'Charlie', 'Hebdo', 'suspects', 'firing', 'security', 'forces'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Explosions', 'gunfire', 'heard', 'scene', 'hostagetaking', 'kosher', 'supermarket', 'eastern', 'Paris', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Hobby', 'Lobby', 'Ferguson', 'PD', 'clarification', 'We', 'chopping', 'someones', 'hand', 'maximum', 'penalty', 'shoplifting'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['It', 'St', 'Louis', 'County', 'police', 'officer', 'shot', 'PIO', 'Brian', 'Schellman', 'ferguson'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Canada', 'Soldier', 'shot', 'MT', 'On', 'Sunday', 'handsome', 'guard', 'picture', 'friend', 'RIP', 'Nathan', 'Cirillo', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Australian', 'police', 'cordon', 'Sydney', 'street', 'reports', 'hostages', 'cafe', 'Flag', 'window', 'ISIS', 'militants'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['RT', 'The', 'STL', 'SWAT', 'team', 'weapons', 'drawn', 'Ferguson', 'protest', 'broad', 'daylight', 'media', 'attending', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['ctvottawa', 'confirms', 'separate', 'shootings', 'One', 'Parliament', 'Hill', 'National', 'War', 'Memorial', 'Rideau', 'Centre'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['sydneysiege', 'A', 'loud', 'blasts', 'bursts', 'ammunition', 'heard', 'cafe', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['RIPNathanCirilloRT', 'Soldier', 'killed', 'war', 'memorial', 'identified', 'Cpl', 'Nathan', 'Cirillo', 'OttawaShooting', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Ottawa', 'police', 'confirm', 'suspect', 'Parliament', 'Hill', 'shooting', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['killed', 'Paris', 'Alu', 'Akhbar', 'shouting', 'armed', 'attack', 'satirical', 'mag', 'Charlie', 'Hebdo', 'Background', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['One', 'suspect', 'CharlieHebdo', 'shooting', 'handed', 'remain', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Gutwrenching', 'soldier', 'guarding', 'national', 'war', 'memorial', 'died', 'Rest', 'peace', 'Cpl', 'Nathan', 'Cirillo', 'OttawaShooting', 'NeverForget'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Reports', 'SydneySiege', 'gunman', 'speak', 'Tony', 'Abbott', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Gunfire', 'explosions', 'heard', 'armed', 'police', 'stormed', 'Lindt', 'cafe', 'building', 'Sydney', 'sydneysiege'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['US', 'Army', 'increases', 'security', 'Tomb', 'Unknown', 'Soldier', 'Arlington', 'Natl', 'Cemetery', 'precaution', 'Ottawa', 'shootings'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['After', 'minutes', 'Chief', 'Jackson', 'abruptly', 'press', 'questions', 'yelled', 'MikeBrown', 'Ferguson'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['This', 'stand', 'victims', 'condemn', 'terror', 'remind', 'Breivik', 'Mcveigh', 'CharlieHebdo'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Theres', 'anger', 'selfies', 'sydneysiege', 'cafe', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Thoughts', 'prayers', 'hometown', 'Ottawa'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['A', 'siege', 'Sydney', 'trend', 'jihadist', 'violence', 'reached', 'Australia', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Sydney', 'bad', 'hostage', 'situation'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Tonights', 'Maple', 'Leafs', 'Senators', 'postponed', 'shootings', 'Ottawa', 'morning', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Uber', 'Is', 'Allegedly', 'Charging', 'Passengers', 'Minimum', 'During', 'Sydney', 'Siege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Sydneysiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['What', 'Sheikh', 'Haron', 'suspected', 'hostage', 'taker', 'SydneySiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Thoughts', 'prayer', 'involved', 'tragic', 'events', 'morning', 'Ottawa'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Children', 'hostage', 'kosher', 'store', 'reports', 'ParisAttacks', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Thoughts', 'Ferguson', 'We', 'collectively', 'stand', 'senseless', 'violence', 'confronts', 'youth', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['VIDEO', 'Australian', 'police', 'talking', 'gunman', 'holding', 'hostages', 'cafe', 'Sydney', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'A', 'confirms', 'CP', 'deceased', 'soldier', 'Ottawa', 'Cpl', 'Nathan', 'Cirillo', 'Cirillo', 'Hamilton', 'Argylls'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Hostages', 'Sydney', 'Australia', 'cafe', 'Islamic', 'flag', 'displayed', 'window', 'local', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Minutes', 'silence', 'France', 'remember', 'victims', 'Paris', 'shooting', 'JeSuisCharlie', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Let', 'pray', 'hostages', 'MartinPlace', 'Sydney', 'police', 'handling', 'situation', 'peaceful', 'resolution'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['The', 'violence', 'OttawaShooting', 'city', 'edge', 'After', 'chaos', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Sydneysiege', 'Police', 'direct', 'contact', 'gunman', 'precise', 'hostages', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Germanwings', 'confirm', 'distress', 'signal', 'flight', 'U', 'crashed', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['First', 'MikeBrown', 'accused', 'shoplifting', 'Now', 'robbery', 'Its', 'telling', 'Ferguson', 'PD', 'spewing', 'lie', 'lie'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Parisians', 'gather', 'Notre', 'Dame', 'mourn', 'CharlieHebdo', 'killings', 'Jesuischarlie', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Third', 'defending', 'rights', 'Charlie', 'Hebdo', 'defended', 'defending', 'speech', 'attacking', 'Muslims'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Very', 'tense', 'situation', 'Ottawa', 'morning', 'Multiple', 'gun', 'shots', 'fired', 'caucus', 'safe', 'lockdown', 'Unbelievable'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['CBC', 'News', 'Ottawa', 'independently', 'confirmed', 'gunman', 'shot', 'killed', 'Michael', 'ZehafBibeau', 'cbcOTT', 'OTTnews'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Germanwings', 'CEO', 'Germans', 'Spanish', 'citizens', 'Americans', 'plane', 'crashed', 'France', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Hypocrites', 'screen', 'shot', 'CharlieHebdo', 'PK', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['The', 'AFP', 'hostage', 'taker', 'monitoring', 'social', 'media', 'Guardian', 'Australia', 'sources', 'sydneysiege'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Artists', 'fight', 'violence', 'images', 'CharlieHebdo', 'solidarity', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Ottawa', 'police', 'Actively', 'suspects', 'Canadian', 'parliament', 'shooting', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Reports', 'shots', 'fired', 'Parliament', 'Hill'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['believed', 'hostages', 'Paris', 'market', 'tells', 'BFMTV', 'Interior', 'Ministry', 'confirmed', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['RT', 'RCMP', 'advises', 'downtown', 'Ottawa', 'windows', 'roofs', 'ongoing', 'police', 'incident'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Banksy', 'JeSuisCharlie', 'beautiful', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Someone', 'eyes', 'checkthedatestamp', 'photo', 'Isnt', 'JUNE', 'Ferguson', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['controversial', 'covers', 'CharlieHebdo', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'French', 'police', 'convoy', 'helicopters', 'rush', 'scene', 'operation', 'detain', 'CharlieHebdo', 'shooting', 'suspects', 'AP'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Ferguson', 'police', 'chief', 'announced', 'officer', 'Darren', 'Wilson', 'shot', 'unarmed', 'teen', 'Michael', 'Brown'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['This', 'exclusion', 'zone', 'Sydneys', 'CBD', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['French', 'officials', 'alleged', 'gunman', 'ID', 'getaway', 'CharlieHebdo', 'attack', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'confirms', 'PAX', 'Crew', 'board'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Hes', 'blame', 'menHe', 'gun', 'blame', 'gunownersHes', 'Muslim', 'blame', 'Muslims', 'sydneysiege'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'One', 'gunman', 'killed', 'shooting', 'Parliament', 'Hill', 'sources', 'confirm', 'CTV', 'News'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Paramedics', 'carrying', 'hostages', 'MartinPl', 'SydneySiege', 'News'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Police', 'officers', 'hold', 'moment', 'silence', 'remember', 'killed', 'CharlieHebdo', 'attack', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['The', 'Germanwings', 'AirbusA', 'headed', 'Barcelona', 'Spain', 'Dusseldorf', 'Germany', 'French', 'Prime', 'Minister', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Well', 'forget', 'brave', 'women', 'protect', 'capital', 'Thank', 'Ottawa', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['As', 'involved', 'SydneySiege', 'Praying', 'wake', '‚ù§'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Reports', 'explosion', 'kebab', 'mosque', 'Lyon', 'No', 'indication', 'link', 'CharlieHebdo', 'attack', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Snipers', 'Ottawas', 'National', 'Art', 'Gallery', 'gunfire', 'parliament', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['WATCH', 'The', 'moment', 'women', 'escaped', 'sydneysiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Sydney', 'Hostage', 'taker', 'demands', 'ISIS', 'flag', 'warns', 'bombs', 'city', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Extended', 'Video', 'Guns', 'raised', 'officers', 'rush', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['No', 'surprise', 'In', 'July', 'windowsmashing', 'rioters', 'stormed', 'Paris', 'chanting', 'gas', 'Jews', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Police', 'confirm', 'shooting', 'Rideau', 'Centre', 'mall', 'downtown', 'Ottawa', 'east', 'Parliament', 'Hill', 'cdnpoli'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Hostage', 'siege', 'selfies', 'RT', 'The', 'punters', 'selfies', 'Lindt', 'Cafe', 'sydneysiege', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['The', 'terrorist', 'attack', 'Paris', 'obscenity', 'It', 'violent', 'assault', 'freedom', 'media', 'freed', 'expression', 'peoples'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['carried', 'CharlieHebdo', 'attack', 'authorities', 'searching', 'French', 'interior', 'minister', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Another', 'hostage', 'lindtcafe', 'sydneysiege', 'demands', 'gunmen', 'blacked', 'media', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Rideau', 'Centre', 'manager', 'shooting', 'mall', 'But', 'leave', 'enter', 'OttawaShooting'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['They', 'bring', 'France', 'knees', 'stand', 'JeSuisCharlie', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'NEWS', 'PHOTO', 'GALLERY', 'Parliament', 'Hill', 'attacked', 'soldier', 'shot', 'National', 'War', 'Memorial', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['At', 'hostages', 'kosher', 'supermarket', 'eastern', 'Paris', 'reports', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Reports', 'Sydney', 'Australia', 'cafe', 'ISIS', 'flag', 'displayed', 'hostages', 'window', 'See', 'tweets'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Hostages', 'Sydney', 'cafe', 'Islamic', 'flag', 'window', 'local', 'TV', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['AP', 'VIDEO', 'Gunman', 'shoots', 'Canadian', 'soldier', 'standing', 'guard', 'National', 'War', 'Memorial', 'Ottawa', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['AirbusCrash', 'According', 'Germanwings', 'plane', 'crashed', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['French', 'media', 'calling', 'CharlieHebdo', 'attack', 'attack', 'France', 'decades', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Cops', 'forcing', 'legally', 'stand', 'sidewalk', 'Ferguson'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Armed', 'French', 'Police', 'At', 'Trocadero', 'In', 'Paris', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['From', 'Martin', 'Place', 'newsroom', 'gunman', 'rotating', 'hostages', 'forcing', 'stand', 'windows', 'hours'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['NBCNews', 'demand', 'retract', 'lie', 'Ferguson', 'shouting', 'Kill', 'police', 'Reporters', 'scene', 'refuted'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Still', 'conflicting', 'reports', 'death', 'witness', 'Police', 'Tempers', 'rising', 'Ferguson', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Smoke', 'building', 'Charlie', 'Hebdo', 'attack', 'suspects', 'located', 'WATCH', 'LIVE', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Frances', 'interior', 'minister', 'operation', 'detain', 'Charlie', 'Hebdo', 'suspects', 'underway', 'northeast', 'Paris', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'Both', 'Charlie', 'Hebdo', 'suspects', 'killed', 'police', 'storm', 'building', 'police', 'sources', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['The', 'person', 'killed', 'Charlie', 'Hebdo', 'attacks', 'Muslim', 'police', 'officer', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['journalists', 'massacres', 'Israel', 'Gaza', 'Where', 'freedom', 'speech', 'CharlieHebdo', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'hostages', 'escaping', 'sydneysiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Last', 'Germanwings', 'A', 'distress', 'emergency', 'emergency', 'authorities', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['French', 'Prime', 'Minister', 'confirming', 'plane', 'crash', 'U'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['UPDATE', 'hostages', 'Sydney', 'coffee', 'Sydney', 'Opera', 'House', 'Abbott', 'deliver', 'statement', 'shortly', 'ISISAttacks', 'sydneyseige'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Things', 'ugly', 'A', 'racist', 'bloke', 'yelling', 'rest', 'crowd', 'Muhammed', 'pedophile', 'rapist', 'sydneysiege', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Ferguson', 'cops', 'beat', 'innocent', 'charged', 'bleeding', 'uniforms', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Reports', 'police', 'identified', 'hostage', 'taker', 'SydneySiege', 'Five', 'hostages', 'escaped', 'believed'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['The', 'copilot', 'Germanwings', 'Airbus', 'convert', 'Islam', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['NORAD', 'increases', 'planes', 'alert', 'status', 'ready', 'respond', 'official', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Total', 'hostages', 'Lindt', 'MartinPlace', 'women', 'LIVE', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Sydney', 'Australia', 'Not', 'technically', 'ISIS', 'flag', 'Shahadah', 'flag', 'Islamic', 'creed', 'Commonly', 'militants', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Canada', 'Michael', 'ZehafBibeau', 'suspect', 'Ottawa', 'shooting', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['If', 'Germanwings', 'copilot', 'Lubitz', 'Muslimit', 'wud', 'hv', 'classic', 'terrorism', 'caseBut', 'brownso', 'mental', 'heath'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Paris', 'gunman', 'vows', 'kill', 'hostages', 'France', 'storms', 'printing', 'plant', 'brothers', 'accused', 'attacking', 'newspaper', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['If', 'Sydney', 'Texas', 'customers', 'drawn', 'guns', 'moment', 'ISIS', 'terrorist', 'walked', 'A'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'Soldier', 'shot', 'Canadian', 'parliament', 'authorities', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['If', 'Sydney', 'err', 'safety', 'Also', 'cautious', 'Lindt', 'cafe', 'diversion'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Exactly', 'Why', 'release', 'stopping', 'Mike', 'Brown', 'robbery', 'JESUS', 'CHRIST', 'THIS', 'IS', 'BAD', 'Ferguson'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Apparent', 'hostage', 'situation', 'unfolding', 'Sydney', 'prime', 'minister', 'convenes', 'security', 'briefing', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['babies', 'passengers', 'board', 'crashed', 'Germanwings', 'plane', 'briefing', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Dear', 'confused', 'rightwing', 'friends', 'article', 'Mondays', 'incident', 'Ottawa', 'shooting', 'morning', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['SYDNEYSIEGE', 'OVER', 'police', 'confirm', 'Reports', 'hostagetaker', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Ottawa', 'police', 'confirm', 'male', 'shooting', 'suspect', 'died', 'custody', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Incredible', 'shot', 'A', 'woman', 'mouthtomouth', 'fallen', 'soldier', 'War', 'Memorial', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['FACT', 'We', 'MikeBrown', 'allegedly', 'stole', 'cheap', 'cigars', 'Ferguson', 'police', 'officer', 'shot', 'killed'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['A', 'flight', 'U', 'registration', 'DAIPX', 'lost', 'feet', 'UTC', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['JUST', 'IN', 'Ferguson', 'police', 'identify', 'officer', 'shot', 'Michael', 'Brown', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['A', 'French', 'police', 'officer', 'suspected', 'Charlie', 'Hebdo', 'gunmen', 'martyrs', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['These', 'hostages', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Ferguson', 'PD', 'charged', 'HenryDavis', 'destruction', 'property', 'bleeding', 'uniforms', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['When', 'MikeBrown', 'shot', 'police', 'officer', 'Darren', 'Wilson', 'Ferguson', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Brilliant', 'CharlieHebdo', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['No', 'word', 'gunman', 'Man', 'Monis', 'SydneySiege', 'News', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Ferguson', 'police', 'chief', 'robbery', 'DOES', 'NOT', 'initial', 'contact', 'Darren', 'Wilson', 'Michael', 'Brown'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Top', 'Qatar', 'urges', 'Muslims', '‚Äò', 'apologize', 'CharlieHebdo', 'ParisShooting', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Reports', 'Alleged', 'Sydney', 'hostage', 'taker', 'charged', 'accessory', 'exwifes', 'murder', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['Line', 'police', 'cars', 'beams', 'greets', 'enter', 'Ferguson', 'Its', 'shut', 'No', 'media', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['All', 'European', 'flags', 'fly', 'halfmast', 'pay', 'tribute', 'victims', 'terrorist', 'attack', 'Paris', 'URL'] ,  lable:  1 \n",
      "\n",
      "test_data:  ['BREAKING', 'Anonymous', 'audio', 'files', 'police', 'dispatch', 'EMS', 'MikeBrown', 'shooting', 'Will', 'release', 'ASAP', 'Ferguson'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['BREAKING', 'UPDATE', 'hostages', 'leave', 'Sydneysiege', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Germanwings', 'flight', 'U', 'crashes', 'French', 'Alps', 'plane', 'altitude', 'mayday', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Police', 'confirm', 'sydneysiege', 'Two', 'reportedly', 'injured', 'suffering', 'gunshot', 'wounds', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['The', 'US', 'Consulate', 'Sydney', 'evacuated', 'Statement', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Banksys', 'CharlieHebdo', 'URL'] ,  lable:  0 \n",
      "\n",
      "test_data:  ['Looks', 'Charlie', 'Hebdo', 'cartoonist', 'killed', 'Charb', 'AlQaedas', 'hit', 'list', 'Cabu', 'Wolinski', 'Tignous'] ,  lable:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(test_result)):\n",
    "    if int(test_result[i][1]) != test_data[i][1]:\n",
    "        print('test_data: ', srctweet_clean[i], ', ', 'lable: ', test_result[i][1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
